{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Ch2&3_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianshin12/19-lab/blob/master/RL_Ch2%263_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISGqpSpuCMU2",
        "colab_type": "text"
      },
      "source": [
        "# 2장 강화학습 개념"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gcpHd086OJi",
        "colab_type": "text"
      },
      "source": [
        "# 2.1 강화학습 개요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbvy2jPE6OHn",
        "colab_type": "text"
      },
      "source": [
        "**강화학습**은 일반적으로 불연속적인 **이산시간을 시간변수**로 하여, 시간 간격마다 **행동**이 가해지는 순차적인 **의사 결정 문제**를 구성한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTb3bbtR6OB9",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 구성요소"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gM_Tncx6lH3",
        "colab_type": "text"
      },
      "source": [
        "* 에이전트 : 의사 결정 주체\n",
        "* 환경 : 에이전트를 제외한 요소(시스템)\n",
        "* 행동\n",
        "* 상태\n",
        "* 보상"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai6Gn3X76sd8",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op7Owxd66uXw",
        "colab_type": "text"
      },
      "source": [
        "먼저, 에이전트가 **상태를 관측**하여 **정책**(특정 행동을 택하게 하는 확률적인 규칙)에 따라 여러 행동 중 특정한 **행동**을 하여 **환경**을 바꾸어 **새로운 상태**에 도달하며, 이에 따라 **보상**을 받는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHm3xXga7He5",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 목표"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V28Mh4Fh7KwV",
        "colab_type": "text"
      },
      "source": [
        "강화학습이 종료되는 시점까지 **누적된 총 보상을 최대화**하는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYpwsutt7cvo",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 특징"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFnef_nP7a83",
        "colab_type": "text"
      },
      "source": [
        "시간 간격마다 최대의 보상을 얻는 행동을 선택하는 것이 아니라, 총 누적 보상이 최대가 되는 행동을 선택하는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl7fU9hR6hth",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 2.2 강화학습 프로세스 및 표기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42HECaiqUD4u",
        "colab_type": "text"
      },
      "source": [
        "## 강화학습 프로세스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFa6UBGCUg7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-5.jpg?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oJxwyR29eiq",
        "colab_type": "text"
      },
      "source": [
        "1. 에이전트는 **환경의 상태**($X_t$)를 **측정**\n",
        "2. **정책**으로 인해 **선택된 행동**은 **환경에 인가**\n",
        "3. 행동에 의해 환경의 상태는 **다음 상태**($X_{t+1}$)로 **전환**\n",
        "4. 전환된 환경의 상태로부터 **새로운 행동 선택**\n",
        "5. 전환된 환경으로부터 얻는 **보상을 통해 정책을 계속 개선**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTZCSQhS9nqs",
        "colab_type": "text"
      },
      "source": [
        "## 강화학습 표기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVxWFI1r9qxl",
        "colab_type": "text"
      },
      "source": [
        "* 이산공간의 상태 :  $S_t$\n",
        "* 이산공간의 행동 : $a_t$\n",
        "\n",
        "* 연속공간의 상태 : $X_t$\n",
        "* 연속공간의 행동 : $u_t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5L4h-Cm-2Yx",
        "colab_type": "text"
      },
      "source": [
        "# Markov Process\n",
        "MDP를 설명하기 전에 MDP의 기본이 되는 MP와 MRP에 대하여 알아보겠다.  \n",
        "\n",
        "* Markov Property  \n",
        "$S_t \\rightarrow S_{t+1}$로 갈 확률 = $S_1, S_2, S_3, \\cdots S_t$을 거쳐 $S_{t+1}$로 갈 확률  \n",
        "  1. 과거를 버릴 수 있다.\n",
        "  2. State가 History의 모든 관련 정보를 갖고 있다. $\\rightarrow$ state를 아는 순간 history는 필요 없다.\n",
        "  3. State는 미래에 대한 충분한 통계적 표현형이다.  \n",
        "\n",
        "## Markov Process\n",
        "Markov Process는 S (state들의 set), P (state들의 전이확률)로 구성  \n",
        "이산공간인 n개의 State를 옮겨다니며 샘플링이 가능  \n",
        "\n",
        "Ex)  \n",
        "![image](https://user-images.githubusercontent.com/53211502/82746092-02d83d00-9dc7-11ea-8ac8-01466eead3b7.png)  \n",
        "각 상태들로의 전이 확률을 행열로 만들 수 있음.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/53211502/82746191-03bd9e80-9dc8-11ea-81f3-47d629fda1e1.png)  \n",
        "\n",
        "* 샘플링  \n",
        "episodes : 어느 State에서 시작하여 Terminal State까지  \n",
        "1. C1 $\\rightarrow$ C2 $\\rightarrow$ C3 $\\rightarrow$ Pass $\\rightarrow$ Sleep\n",
        "2. C1 $\\rightarrow$ Facebook $\\rightarrow$ Facebook $\\rightarrow$ C1 $\\rightarrow$ C2 $\\rightarrow$ Sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8tVtq3dG3pI",
        "colab_type": "text"
      },
      "source": [
        "## Markov Reward Process\n",
        "Markov Reward Process는 S(state), P(전이확률), R(어떤 State에 도달하면 주어지는 보상), $\\gamma$(discount factor, 감가율)로 구성  \n",
        "\n",
        "![image](https://user-images.githubusercontent.com/53211502/82746776-919c8800-9dce-11ea-8855-184c0a106eb5.png)\n",
        "\n",
        "* Return  \n",
        "경로까지 포함하여 받을 수 있는 Reward의 총합  \n",
        "강화학습의 목표는 Reward의 최대화가 아닌 Return의 최대화  \n",
        "단순 합이 아닌 미래의 보상은 $\\gamma^1, \\gamma^2, \\gamma^3 \\cdots $ 감가율을 곱한 후 합  \n",
        "$\\gamma=0$이면 근시안, 즉 가까운 Reward을 최대로 취하려 한다.  \n",
        "$\\gamma=1$이면 반대로 Reward에 대하여 멀리 볼 수 있다.\n",
        "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0} ^\\infty \\gamma^k R_{t+k+1}$$  \n",
        "* 왜 감가율을 사용하는가  \n",
        "1. 수학적으로 편리해서 $\\rightarrow$ 수렴성을 무한 수열의 합으로 증명 가능\n",
        "2. 모든 sequence가 종료되는게 보장되면 $\\gamma=1$로 설정해도 될 수도 있다.  \n",
        "* 가치함수, Value function  \n",
        "Return의 기댓값  \n",
        "$V(s) = E[G_t|S_t=s]$\n",
        "State에서 계속 episode를 생성한 후 각 episode의 Return 값을 평균  \n",
        "Ex) $ S_1 = C_1 , \\gamma =0.5$  \n",
        "C1, C2, C3, Pass, Sleep $\\rightarrow G_1=-2-2*0.5-2*0.5^2+10*0.5^3$  \n",
        "C1, FB, FB, C1, C2, Sleep $\\rightarrow G_1=-2-1*0.5-1*0.5^2-2*0.5^3-2*0.5^4$  \n",
        "등등 C1으로 시작하는 에피소드의 Return값을 평균 $\\rightarrow$ $C_1$에 도달하였을 때 어느 정도 Return값을 받을지에 대한 기댓값"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DSQJRCNQdew",
        "colab_type": "text"
      },
      "source": [
        "* Bellman Equation  \n",
        "$V(s)=E[G_t|S_t=s ]$  \n",
        "$\\ \\ \\ \\ \\ \\ \\ \\ =E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\cdots|S_t=s]$  \n",
        "$\\ \\ \\ \\ \\ \\ \\ \\ =E[R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+\\cdots)|S_t=s]$  \n",
        "$\\ \\ \\ \\ \\ \\ \\ \\ =E[R_{t+1}+\\gamma G_{t+1}|S_t=s]$  \n",
        "$\\ \\ \\ \\ \\ \\ \\ \\ =E[R_{t+1}+\\gamma V(S_{t+1})|S_t=s]$  \n",
        "어느 State에서의 Value = 한 스텝 후 $R_{t+1}$ + 다음 State에서의 Value  \n",
        "현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계 방정식이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiqYq0_HFCfA",
        "colab_type": "text"
      },
      "source": [
        "# 2.3 마르코프 결정 프로세스(MDP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NbGXMQIFHOJ",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT4zoBTFFHKr",
        "colab_type": "text"
      },
      "source": [
        "### **마르코프 결정 프로세스(MDP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw_uvuefpGGY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        " : 상태($X_t$), 상태천이 확률밀도함수($p$), 행동($U_t$), 보상함수($r(X_t,U_t)$)로 이루어진 이산시간 확률 프로세스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN97EehRumlq",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1.1 확률적 MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1wJHC09urFU",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$  환경 모델이 상태천이 확률밀도함수로 주어지는 경우 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhLYQ85XFHHQ",
        "colab_type": "text"
      },
      "source": [
        "### **상대천이 확률밀도함수**($p$) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2VaKdyfpKIR",
        "colab_type": "text"
      },
      "source": [
        ": 어떤 상태($X_t$)에서 에이전트가 행동($U_t$)을 했을 떄, 다음 상태($X_{t+1}$)로 갈 확률밀도함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6o_QiYFHDk",
        "colab_type": "text"
      },
      "source": [
        "연속공간 상태 및 행동변수 -> $p(X_{t+1} \\mid X_t, U_t) $(확률밀도함수)\n",
        "<br> 이산공간 상태 및 행동변수 -> $P(X_{t+1} \\mid X_t, U_t) $(확률)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvDpowdkFG_o",
        "colab_type": "text"
      },
      "source": [
        "이 함수는 미래의 상태(다음의 상태)가 과거의 상태와 행동에 관계없이 오직 현재 상태와 행동에만 영향을 받기 때문에, 마르코프 시퀀스이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-o-BAQdo5-G",
        "colab_type": "text"
      },
      "source": [
        "### **조건부 확률밀도함수($\\pi$)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T76zrtpSow80",
        "colab_type": "text"
      },
      "source": [
        ": MDP에서 **누적된 보상**을 **가장 많이 획득**하기 위해 각 상태에서 **어떤 행동을 취할 것인지**를 나타내는 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-jBTRybow5V",
        "colab_type": "text"
      },
      "source": [
        "$$\\pi(U_t \\mid X_t) =p(U_t \\mid X_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbjWnr2oow3A",
        "colab_type": "text"
      },
      "source": [
        "$\\pi(U_t \\mid X_t)$ 를 **정책** 이라고 하며, **한 상태변수**에서 **여러개의 행동**을 **선택** 할 수 있으며, 각 행동의 확률밀도함수에 의해 선택된다.\n",
        "<BR>이를 다른말로 **확률적 정책**이라고도 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqyOxDYdow0Y",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427077-2aa78e00-9195-11ea-8317-5a97c44db091.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC3J6W47owxd",
        "colab_type": "text"
      },
      "source": [
        "위 그림은 상태변수 $X_0$에서 어떤 정책 $\\pi$에 의해 행동 $U_0$가 확률적으로  선택되면, 상태천이 확률밀도함수 $p(X_{1}\\mid X_0,U_0)$에 의해 $X_1$으로 이동하고 이때, 보상 $r(X_0,U_0)$이 주어지며 이 과정이 반복되는 MDP 전개 그림이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFhlYs4owud",
        "colab_type": "text"
      },
      "source": [
        "**궤적($\\tau$)** : **상태변수**와 **행동**의 **연속**적인 **시퀀스**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67E2pk2Itsmc",
        "colab_type": "text"
      },
      "source": [
        "$$\\tau = (X_0,U_0,X_1,U_1,\\ldots,X_\\tau,U_\\tau)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuYiQ5n3u3Bq",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1.2 확정적 MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iN8SPrHt_hS",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 환경모델과 정책이 모두 확정적으로 주어지는 경우"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwnNlL7-vL7l",
        "colab_type": "text"
      },
      "source": [
        "**환경 모델** :  $X_{t+1}=f(X_t,U_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUCQlFxuuT18",
        "colab_type": "text"
      },
      "source": [
        "이는, 시간스텝 $t$에서 상태와 행동이 주어지면 다음상태를 확정적으로 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDrkgRQfuT4-",
        "colab_type": "text"
      },
      "source": [
        "**정책** : $U_t=\\pi(X_t)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnsGhWLuTy5",
        "colab_type": "text"
      },
      "source": [
        "정책은 상태변수에서 행동을 **직접 계산**한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3jxt9TfuTv_",
        "colab_type": "text"
      },
      "source": [
        "### **반환값($G_t$)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs2OnG_2uTtF",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 시간스텝 $t$일 때, 얻을 수 있는 **보상의 총합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ym3_U1uTpI",
        "colab_type": "text"
      },
      "source": [
        "$$G_t=\\sum_{k=t}^T  \\gamma^{k-t}r(X_k,U_k)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFsU-AW8uTmM",
        "colab_type": "text"
      },
      "source": [
        "$\\gamma \\in [0,1]$은 **감가율**이다. 값이 작을수록 보상을 가까운 미래에 받는다. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXd7d-Pn_ab7",
        "colab_type": "text"
      },
      "source": [
        "$\\gamma$=0이면, $G_t=r(X_t,U_t)$이므로, 오직 가까운 미래에 대해 받는 보상이다.\n",
        "<br> $\\gamma$=1이면, $G_t=\\sum_{k=t}^T r(X_k,U_k)$이므로, 먼 미래에 대해 받는 보상과 연관이 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrEi5fdYuTjw",
        "colab_type": "text"
      },
      "source": [
        "### **에피소드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t87CTO45uThU",
        "colab_type": "text"
      },
      "source": [
        ": 어떤 정책을 시행해 $X_0 \\rightarrow U_0 \\rightarrow r(X_0,U_0)\\rightarrow \\ldots\\rightarrow X_T\\rightarrow U_T$의 순서로 전개될 때, 이러한 **상태변수, 행동, 보상의 시퀀스 집합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua6EdqpnuTd5",
        "colab_type": "text"
      },
      "source": [
        "유한 구간 에피소드 : 특정 상태변수에 도달하는 등 목적이 성취되면 종료되는 에피소드($t=T$)\n",
        "<BR>무한 구간 에피소드 : $T=\\infty$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ek8Ok8WuTYz",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.2 가치함수 - 상태가치 & 행동가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkaLffaouTVw",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2.1 상태가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdHHpkS91BPZ",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427186-66425800-9195-11ea-9d18-f9c1169d1fae.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT2T9BJF0opw",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 상태변수($X_t$)에서 정책 $\\pi$에 의해 **행동이 가해졌을 때**, 기대할 수 있는 **반환값**.  즉, 미래 보상의 총합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDPyV2AN1ZUx",
        "colab_type": "text"
      },
      "source": [
        "상태가치함수 식 : $V^\\pi(X_t)=E_{\\tau_{u_t:u_T \\sim p(\\tau_{u_t:u_T\\mid X_t})}}[\\sum_{k=t}^T \\gamma^{k-t}r(X_k,u_k)\\mid X_t]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A09CoowuTTI",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2.2 행동가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNYRMFgb3Sde",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427917-6d1d9a80-9196-11ea-97ea-f00797babb79.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucMYcvvd3T_R",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 상태변수($X_t$)에서  **행동($u_t$)을 선택하고**, 그로부터 정책($\\pi$)에 의해 행동이 가해졌을 때,기대할 수 있는 **반환값**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_YnhiHN3xwF",
        "colab_type": "text"
      },
      "source": [
        "행동가치 식 :  $Q^\\pi(X_t,u_t)=E_{\\tau_{x_{t+1}:u_T \\sim p(\\tau_{u_{t+1}:u_T\\mid X_t,u_t})}}[\\sum_{k=t}^T \\gamma^{k-t}r(X_k,u_k)\\mid X_t,u_t]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtOEgeko30fI",
        "colab_type": "text"
      },
      "source": [
        "이 두 식을 이용하면 다음과 같은 결론을 얻을 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NnxxRh230cw",
        "colab_type": "text"
      },
      "source": [
        "$V^\\pi(X_t)=E_{u_t \\sim \\pi(u_t \\mid X_t)}[Q^\\pi (X_t,u_t)]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap4ayAWY30aP",
        "colab_type": "text"
      },
      "source": [
        "즉, **상태가치**는 상태변수$X_t$에서 **선택 가능한 모든 행동**$u_t$에 대한 **행동가치의 기댓값(평균값)**이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHqXnNtM30GD",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.3 벨만 방정식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFyaLHQL441I",
        "colab_type": "text"
      },
      "source": [
        ": **현재** 상태변수의 **가치**와 **다음** 시간스텝의 상태변수의 **가치**와의 관게"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukWuzlfkDjRW",
        "colab_type": "text"
      },
      "source": [
        "벨만 방정식은 상태가치 함수와 행동가치 함수로 각각 이루어진 식으로 구분할 수 있다. \n",
        "<br> 두 식 모두 각각의 가치함수에 대해 현재 상태변수 가치와 다음 시간스텝의 상태변수 가치의 관계를 나타낸다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeZhytoa45Qb",
        "colab_type": "text"
      },
      "source": [
        "$V^\\pi(X_t)=E_{u_t \\sim \\pi(u_t \\mid X_t)}[r(X_t,u_t)+E_{x_{t+1} \\sim p(X_{t+1} \\mid X_t,u_t)}[rV^\\pi(X_{t+1})]]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAIIErh045LY",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.4 벨만 최적 방정식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsqt2qUDFJ90",
        "colab_type": "text"
      },
      "source": [
        ": 벨만 방정식과 비슷한 원리로, **현재의 최적 가치**와 **다음 시간스텝의 최적 가치**의 관계를 나타낸 식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQPSsUxJ45I_",
        "colab_type": "text"
      },
      "source": [
        "**최적 상태가치 함수** : 모든 정책 중 상태가치 값을 최대로 만드는 정책(최적 정책)을 적용한 상태가치 함수\n",
        "<br>**최적 행동가치 함수** : 모든 정책 중 상태가치 값을 최대로 만드는 정책(최적 정책)을 적용한 행동가치 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxAXPC3BuTQQ",
        "colab_type": "text"
      },
      "source": [
        "최적 상태가치 함수는 최적 행동가치 함수와 아래와 같은 관계를 갖는다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pcEFNFQCC-B",
        "colab_type": "text"
      },
      "source": [
        "$V^*(X_t)=maxQ^*(X_t,u_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMZV7LygErGo",
        "colab_type": "text"
      },
      "source": [
        "즉, 최적 행동가치 함수의 최댓값이 최적 상태가치 함수이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC_z6YyVCWhI",
        "colab_type": "text"
      },
      "source": [
        "이렇게 상태가치의 값을 최대로 만드는 정책을 **최적 정책**($\\pi^*(u_t \\mid X_t)$) 이라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMsk5m_oCtNH",
        "colab_type": "text"
      },
      "source": [
        "##2.4 강화학습 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbljO-fxHLsi",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 문제 해결 방법들의 공통점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAL2uidkC_vN",
        "colab_type": "text"
      },
      "source": [
        "![2 9](https://user-images.githubusercontent.com/53015968/81430084-d05cfc00-9199-11ea-9deb-00c641339c66.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyDUe99VDA5r",
        "colab_type": "text"
      },
      "source": [
        "1. 정책 실행을 통한 데이터 생성\n",
        "2. 모델 또는 가치함수의 추정\n",
        "3. 정책 개선 \n",
        "<br>1 ~ 3단계 반복 $\\rightarrow$ 최적의 정책 산출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lv_hufnvNnT",
        "colab_type": "text"
      },
      "source": [
        "# 3장 정책 그래디언트  \n",
        "\n",
        "## 3.1 배경  \n",
        "\n",
        "정책 그래디언트 : 정책(policy)를 파라미터화하고, 누적 보상과 정책 파라미터 간의 관계를 구축하고 최적화 방법을 통해 누적 보상을 최대화하는 정책 파라미터를 계산하는 강화학습의 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwjFuVCzvSFy",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 목적함수  \n",
        "\n",
        "목적함수 **J** : 반환값의 기댓값으로 이루어진 함수\n",
        "\n",
        "=> 목표 : $\\theta$ 로 파라미터화된 $\\pi_\\theta(u_t|x_t)$ 을 사용하여 목적함수를 최대로 만드는 $\\theta$ 를 계산하는 것.\n",
        "\n",
        "$$\\theta^*=argmaxJ(\\theta)$$\n",
        "$$J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T \\gamma^t r(x_t,u_t)]   =\\int_\\tau p_\\theta(\\tau)(\\sum_{t=0} ^T \\gamma^t r (x_t, u_t))d\\tau $$  \n",
        "$r(x_t, u_t)$ : 시간스텝 $t$일 때 상태변수가 $x_t$에서 행동 $u_t$를 사용했을 때 에이전트가 받는 보상\n",
        "\n",
        "$\\gamma\\in[0,1]$ : 감가율(discount factor)\n",
        "\n",
        "$p_\\theta(\\tau)$ : 기댓값을 계산할 때 확률밀도함수로 $p_\\theta(\\tau)$를 사용한다는 의미  \n",
        "\n",
        "$\\tau$ : 정책 $\\pi_\\theta$로 생성되는 궤적 $\\tau=(x_0, u_0, x_1, u_1, x_2, u_2, x_3, u_3, \\cdots, x_T, u_T)$\n",
        "\n",
        "$p_\\theta(\\tau)$ : 정책 $\\pi_\\theta$로 생성되는 궤적의 확률밀도함수\n",
        "\n",
        "![KakaoTalk_20200509_145820465](https://user-images.githubusercontent.com/53211502/81465554-c4a71f00-9205-11ea-852f-4aa6c861bb7b.jpg)  \n",
        "\n",
        "- 정책 신경망\n",
        "\n",
        "파라미터 $\\theta$는 그림 3.2에 도시한 것 같이 신경망의 모든 가중치다.\n",
        "![KakaoTalk_20200509_151311299](https://user-images.githubusercontent.com/53211502/81465828-f0c39f80-9207-11ea-968a-db245cc2df55.jpg)\n",
        "\n",
        "- MDP에서의 에이전트의 정책 신경망과 환경의 상호작용\n",
        "![KakaoTalk_20200509_152306979](https://user-images.githubusercontent.com/53211502/81465989-0e453900-9209-11ea-96c8-92d6c716625e.jpg)\n",
        "\n",
        "$G_0 = \\sum_{t=0} ^T \\gamma^t r (x_t, u_t)$ : 시간 스텝 $t=0$부터 에피소드가 종료될 때까지 받을 수 있는 전체 궤적에 대한 감가 보상(discount reward)의 총합 = $G_0$\n",
        "\n",
        " $G_k = \\sum_{k=t}^T \\gamma^{k-t} r (x_k, u_k)$ : 임의의 시간 $k=t$부터 에피소드가 종료될 때까지 받을 수 있는 예정 보상(reward-to-go)의 총합\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfm3_MzivaCM",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 정책 그래디언트  \n",
        "- 목적함수를 최대로 만드는 $\\theta$를 계산\n",
        "  1. 목적함수를 $\\theta$로 미분  \n",
        "$$ {{\\partial J(\\theta)}\\over{\\partial \\theta}}= \\nabla_\\theta J(\\theta) = \\int_\\tau \\nabla_\\theta p_\\theta(\\tau) \\sum_{t=0}^T \\gamma^t r (x_t, u_t) d\\tau=\\int_\\tau p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau) \\sum_{t=0}^T \\gamma^t r (x_t, u_t) d\\tau$$  \n",
        "\n",
        "  2. $\\nabla_\\theta \\log p_\\theta(\\tau) = {{\\nabla_\\theta p_\\theta(\\tau)}\\over{p_\\theta(\\tau)}}$를 이용 \n",
        "$$\\nabla_\\theta \\log p_\\theta(\\tau) = \\nabla_\\theta (\\log p(x_0) + \\sum_{t=0}^T \\log \\pi_\\theta (u_t|x_t) + \\log p(x_{t+1}|x_t, u_t))$$  \n",
        "\n",
        "  3. 좌변의 두번째 항만 $\\theta$의 함수로 단순화\n",
        "$$\\nabla_\\theta \\log p_\\theta(\\tau) =  \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)$$ \n",
        "\n",
        "  4. 목적함수의 그래디언트 식에 대입  \n",
        "$$\\nabla_\\theta J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)(\\sum_{k=0}^T\\gamma^k r (x_k, u_k)))] $$  \n",
        "=> 환경 모델이 필요 없는 모델 프리(model-free) 강화학습 방법\n",
        "\n",
        "- 시간스텝 t<k에서와 무한 구간 에피소드 고려\n",
        "\n",
        "  => 예정 보상에만 감가율을 적용\n",
        "$$\\nabla_\\theta J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)(\\sum_{k=t}^T\\gamma^{k-t} r (x_k, u_k)))] $$  \n",
        " \n",
        "- 경사상승법을 이용하여 $\\theta$ 업데이트\n",
        "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$$  \n",
        "\n",
        "정책 그래디언트에 사용되는 목적함수 그래디언트를 다음과 같다.  \n",
        "* 목적함수 : $J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T \\gamma^t r(x_t,u_t)], \\tau=(x_0, u_0. x_1. u_1, \\cdots, x_T, u_T)$  \n",
        "* 가정 : 확률적 정책, $u_t \\sim \\pi_\\theta (u_t|x_t)$  \n",
        "* 그래디언트 : $\\nabla_\\theta J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)(\\sum_{k=t}^T\\gamma^{k-t} r (x_k, u_k)))] $\n",
        "* 업데이트 : $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$ \n",
        "\n",
        "=> 정책 그래디언트는 파라마터 **$\\theta$를 업데이트 하는 것**\n"
      ]
    }
  ]
}