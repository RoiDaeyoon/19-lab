{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "수풀강원알 4장.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQjtloQP5Jc+Ij3VRb7bWi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianshin12/19-lab/blob/master/%EC%88%98%ED%92%80%EA%B0%95%EC%9B%90%EC%95%8C_4%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "867-vs5DwNrB",
        "colab_type": "text"
      },
      "source": [
        "# 4장 A2C\n",
        "\n",
        "## 4.1 배경\n",
        "REINFORCE 알고리즘 정책의 두 가진 단점은 에피소드가 끝날 때까지 기다려야하는 점과 그래디언트의 분산이 매우 크다는 것이다.  \n",
        "이 두 가지 단점을 개선한 알고리즘이 어드밴티지 액터-크리틱(advantage actor-critic, A2C)이다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa7OKsHEwsbc",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 그래디언트의 재구성  \n",
        "* 목적함수의 그래디언트  \n",
        "$$\\nabla_\\theta J(\\theta) = E_{\\tau \\sim p_\\theta}[\\sum _{t=0} ^T (\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)(\\sum _{k=t} ^T \\gamma^{k-t}r(x_k, u_k)))]$$\n",
        "$$=\\sum _{t=0} ^T \\{E_{\\tau \\sim p_\\theta}[\\gamma^t \\nabla_\\theta \\log  \\pi_\\theta(u_t|x_t)(\\sum _{k=t} ^T \\gamma^{k-t}r(x_k, u_k))]\\}$$\n",
        "\n",
        "$(\\sum _{k=t} ^T \\gamma^{k-t}r(x_k, u_k))$항은 시간스텝 k=t부터 에피소드가 종료될 때까지 받을 수 있는 감가된 예정 보상(reward-to-go)의 총합인 반환값 $G_t$이다. 이 부분을 더 명확하게 규명하는 것이 목적이다.  \n",
        "궤적 $\\tau$를 시간스텝 t를 기준으로 다음과 같이 두 개의 영역으로 분할한다.  \n",
        "$$\\tau = (x_0, u_0, \\cdots, x_t, u_t) \\cup (x_{t+1}, u_{t+1},\\cdots,x_T,u_T)$$\n",
        "$$=\\tau_{x_0 : u_t}\\cup\\tau_{x_{t+1}:u_T}$$\n",
        "\n",
        "이를 바탕으로 목적함수의 그래디언트는 다음과 같이 표현할 수 있다. \n",
        "$$\\nabla_\\theta J(\\theta)=\\sum _{t=0} ^T \\int _{\\tau_{x_0 : u_t}} \\int _{\\tau_{x_{t+1}:u_T}} (\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)(\\sum _{k=t} ^T \\gamma^{k-t}r(x_k, u_k))) p_\\theta (\\tau_{x_0 : u_t}, \\tau_{x_{t+1}:u_T})d\\tau_{x_{t+1}:u_T} d\\tau_{x_0 : u_t}$$\n",
        "$$=\\sum _{t=0} ^T \\int _{\\tau_{x_0 : u_t}} \\int _{\\tau_{x_{t+1}:u_T}} (\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)(\\sum _{k=t} ^T \\gamma^{k-t}r(x_k, u_k))) p_\\theta (\\tau_{x_{t+1}:u_T} | \\tau_{x_0 : u_t})p_\\theta(\\tau_{x_0 : u_t}) d\\tau_{x_{t+1}:u_T} d\\tau_{x_0 : u_t} $$\n",
        "$$=\\sum _{t=0} ^T \\int _{\\tau_{x_0 : u_t}} (\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)[\\int _{\\tau_{x_{t+1}:u_T}}(\\sum _{k=t} ^T \\gamma^{k-t}r(x_k, u_k)) p_\\theta (\\tau_{x_{t+1}:u_T} | \\tau_{x_0 : u_t}) d\\tau_{x_{t+1}:u_T}] p_\\theta(\\tau_{x_0 : u_t}) d\\tau_{x_0 : u_t} $$\n",
        "$$=\\sum _{t=0} ^T (\\int _{(x_t, u_t)} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)Q^{\\pi_\\theta}(x_t, u_t)p_\\theta(x_t,u_t)dx_tdu_t)$$\n",
        "$$=\\sum _{t=0} ^T (E_{x_t \\sim d_\\theta (x_t),u_t \\sim \\pi_\\theta(u_t|x_t)}[\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)Q^{\\pi_\\theta}(x_t, u_t)])$$\n",
        "\n",
        "베이즈 정리, 마르코프 가정을 적절히 적용하여 식을 변형하였다. $\\gamma^t$는 에피소드의 후반부 궤적에 있는 데이터의 이용도를 크게 떨어뜨리는 단점이 있기 때문에 제거하였다.  \n",
        "\n",
        "원래 의 그래디언트식 $\\sum _{t=0} ^T \\{E_{\\tau \\sim p_\\theta}[\\gamma^t \\nabla_\\theta \\log  \\pi_\\theta(u_t|x_t)(G_t)]\\}$과 비교하면 반환값 $G_t$ 대신에 행동가치 함수가 쓰여진 것을 알 수 있다. 행공가치는 상태변수 $x_t$에서 행동 $u_t$를 선택하고 그러부터 정책 $\\pi$에 의해서 행동이 가해졌을 때 기대할 수 있는 미래의 반환값으로서, 정책이 실현되는 시간스텝 t에서의 기댓값이기 때문에 목적함수의 그래디언트를 계산할 때, 에피소드가 끝날 때까지 기다릴 필요가 없다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0b_HP0Zw3yz",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 분산을 감소시키기 위한 방법 \n",
        "분산은 데이터가 평균값으로부터 얼마나 넓게 산재해 있느냐를 나타내는 척도이다. 목적함수 그래디언트의 분산이 크다면 업데이트될 정책 파라미터값이 들쭉날쭉하므로 신경망 학습이 불안정해지고 정책의 불확실성도 커진다. 따라서 목적함수 그래디언트의 분산을 줄이기 위한 방법을 알아본다.  \n",
        "$$\\nabla_\\theta J(\\theta)=\\sum _{t=0} ^T (\\int _{(x_t, u_t)} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)Q^{\\pi_\\theta}(x_t, u_t)p_\\theta(x_t,u_t)dx_tdu_t)$$\n",
        "$Q^{\\pi_\\theta}(x_t, u_t)$대신 어떠한 파라미터 $b_t$를 넣고 $\\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)= {{\\nabla_\\theta \\pi_\\theta(u_t|x_t)}\\over{\\pi_\\theta(u_t|x_t)}}$ 를 적용하면 다음과 같아진다. \n",
        "$$\\nabla_\\theta J(\\theta)=\\sum_{t=0}^T(\\int_{x_t}[\\int_{u_t}\\nabla_\\theta \\pi_\\theta(u_t|x_t)b_tdu_t]p_\\theta(x_t)dx_t)$$\n",
        "$b_t$가 상수이거나 행동 $u_t$의 함수가 아니라고 가정하면 위 식에서 대괄호 안의 식은 다음과 같이 된다.\n",
        "$$\\int_{u_t}\\nabla_\\theta \\pi_\\theta(u_t|x_t)b_tdu_t=b_t\\nabla_\\theta\\int_{u_t}\\pi_\\theta(u_t|x_t)du_t = b_t\\nabla_\\theta(1)=0$$\n",
        "$$\\therefore \\nabla_\\theta J(\\theta)=0$$\n",
        "목적함수 그래디언트 식의 $Q^{\\pi_\\theta}(x_t, u_t)$에서 $b_t$를 빼도 기댓값은 변하지 않는다.\n",
        "$$\\nabla_\\theta J(\\theta)=\\sum _{t=0} ^T (E_{x_t \\sim d_\\theta (x_t),u_t \\sim \\pi_\\theta(u_t|x_t)}[\\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)(Q^{\\pi_\\theta}(x_t, u_t)-b_t)])$$\n",
        "여기서 $b_t$를 베이스라인(baseline)이라 한다. 베이스라인을 도입함으로써 목적함수 그래디언트의 분산을 줄일 수 있다. 일반적으로 상태가치 함수 $V^{\\pi_\\theta}(x_t)$를 베이스라인으로 사용한다. 상태가치 함수는 행동 $u_t$의 함수가 아니기 때문에 베이스라인으로 사용해도 무방하다.  \n",
        "$$\\nabla_\\theta J(\\theta)=\\sum _{t=0} ^T (E_{x_t \\sim d_\\theta (x_t),u_t \\sim \\pi_\\theta(u_t|x_t)}[\\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)(Q^{\\pi_\\theta}(x_t, u_t)-V^{\\pi_\\theta}(x_t))])$$\n",
        "$$\\nabla_\\theta J(\\theta)=\\sum _{t=0} ^T (E_{x_t \\sim d_\\theta (x_t),u_t \\sim \\pi_\\theta(u_t|x_t)}[\\nabla_\\theta \\log \\pi_\\theta(u_t|x_t)(A^{\\pi_\\theta}(x_t, u_t))])$$\n",
        "$A^{\\pi_\\theta}(x_t, u_t)$는 어드밴티지(advantage)함수라고 한다. 어드밴티지 함수는 상태변수 $x_t$에서 선택된 행동 $u_t$가 평균에 비해 얼마나 좋은지를 평가하는 척도로 해석할 수 있다. 이제 목적함수 그래디언트는 행동가치가 아니라 어드밴티지에 비례한다. 어드밴티지의 정의상 그 값이 행동가치보다 작으므로 그래디언트의 분산이 작아질 것으로 기대할 수 있다. 하지만 여기서 문제는 어드밴티지 값을 모른다는 것이다. 결국 분산을 줄이는 문제는 어드밴티지 함수를 얼마나 정확하게 추정하느냐에 달려 있다."
      ]
    }
  ]
}