{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch 4-5.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv+K/SoLyEpCruieyVorH7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianshin12/19-lab/blob/master/Ch_4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0G0An2pMp_y",
        "colab_type": "text"
      },
      "source": [
        "# 4.5 A2C 알고리즘 구현\n",
        "## 4.5.1 테스트 환경  \n",
        "테스트 환경은 OpenAI Gym에서 제공하는 'Pendulum-v0'이다. (https://gym.openai.com/envs/Pendulum-v0/)  \n",
        "에이전트의 목표는 길이가 1인 진자를 위로 수직으로 세워서 오래 유지하는 것이다.에이전트가 측정할 수 있는 파라미터는 수직축 좌표, 수평축 좌표와 각속도이다.  \n",
        "## 4.5.2 코드 개요  \n",
        "전체적인 학습 프로세스를 구체적으로 살펴보겠다.  \n",
        "> 1. 상태변수, 행동, 시간차 타깃, 어드밴티지($x_i, u_i, y_i, A_i)$를 초기화한다.  \n",
        "batch_states, batch_action, batch_td_targets, batch_advantages = [  ], [  ], [  ], [  ]  \n",
        "\n",
        "> 2. 환경을 초기화하고 환경으로부터 첫 번째 상태변수 $x_0$를 측정한다.  \n",
        "state = self.env.reset()  \n",
        "\n",
        "> 3. 액터 신경망을 이용해 행동 $u_0\\sim\\pi_\\theta(u_0|x_0)$를 확률적으로 선택한다.  \n",
        "action = self.actor.get_action(state)  \n",
        "\n",
        "> 4. 행동이 범위 [-2, 2]를 벗어나지 않도록 제한한다.  \n",
        "action = np.clip(action, -self.action_bound, self.action_bound)  \n",
        "\n",
        "> 5. 행동 $u_0$를 실행해 보상 $r(x_0, u_0)$와 다음 상태변수 $x_1$을 얻는다. 여기서 done=1이면 에피소드가 종료되는 조건에 도달했음을 의미한다.  \n",
        "next_state, reward, done, _ = self.env.step(action)  \n",
        "\n",
        "> 6. Gym 환경과 학습 환경에서 사용하는 변수의 배열 모양이 다름을 고려하여 상태변수, 행동, 보상, 다음 상태변수의 배열 모양을 바꿔준다.  \n",
        "state = np.reshape(state, [1, self.state_dim])  \n",
        "next_state = np.reshape(next_state, [1, self.state_dim])  \n",
        "action = np.reshape(action, [1, self.action_dim])  \n",
        "reward = np.reshape(reward, [1, 1])  \n",
        "\n",
        "> 7. 크리틱 신경망을 이용해 상태가치 $V_\\phi(x_0)$와 다음 상태가치 $V_\\phi(x_1)$을 계산한다.  \n",
        "v_value = self.critic.model.predict(state)  \n",
        "next_v_vale = self.critic.model.predict(next_state)  \n",
        "\n",
        "> 8. 어드밴티지 $A_\\phi(x_0, u_0)=r(x_0, u_0) + \\gamma V_\\phi(x_1) - V_\\phi(x_0)$와 시간차 타깃 $y_0=r(x_0, u_0) + \\gamma V_\\phi(x_1)$을 계산한다. 여기서 학습용으로 사용할 보상의 범위를 [-16, 0]에서 [-1, 1]로 조정한다.  \n",
        "train_reward = (reward+8)/8  \n",
        "advantage, y_i = self.advantage_td_target(train_reward, v_value, next_v_value, done)  \n",
        "\n",
        "> 9. 상태변수, 행동, 시간차 타깃, 어드밴티지 $(x_0, u_0, y_0, A_0)$를 배치에 저장한다.  \n",
        "batch_state.append(state)  \n",
        "batch_state.append(action)  \n",
        "batch_state.append(y_i)  \n",
        "batch_state.append(advantage)  \n",
        "\n",
        "> 10. 배치가 N개 (BATCH_SIZE)만큼 쌓일 때까지는 학습하지 않고 저장만 한다.  \n",
        "if len(batch_size) < self.BATCH_SIZE:\n",
        "  state = next_state[0]  \n",
        "  episode_reward += reward[0]  \n",
        "  time += 1  \n",
        "  continue  \n",
        "\n",
        "> 11. 배치가 차면 각각 N개의 상태변수, 행동, 시간차 타깃, 어드밴티지$(x_i, u_i, y_i, A_i)_{i=1,\\cdots ,N}$을 추출한다. 그리고 배치를 비운다.  \n",
        "states = self.unpack_batch(batch_size)  \n",
        "actions = self.unpack_batch(batch_action)  \n",
        "td_targets = self.unpack_batch(batch_td_target)  \n",
        "advantages = self.unpack_batch(batch_advantage)  \n",
        "batch_states, batch_action, batch_td_targets, batch_advantages = [  ], [  ], [  ], [  ]  \n",
        "\n",
        "> 12. 손실함수 $L= {1 \\over 2N} \\sum _i (y_i-V_\\phi(x_i))^2$를 이용해 크리틱 신경망을 학습한다.  \n",
        "self.critic.train_on_batch(states, td_targets)  \n",
        "\n",
        "> 13. 목적함수 그래디언트 $\\nabla_\\theta J(\\theta) \\approx \\nabla_\\theta \\sum_i(log \\pi_\\theta(u_i|x_i)A_\\phi(x_i, u_i))$를 이용해 액터 신경망을 학습한다.  \n",
        "self.actor.train(states, actions, advantages)  \n",
        "\n",
        "> 14. 다시 상태변수 $x_i$를 이용해 행동 $u_i$를 계산하는 과정을 되풀이한다.  \n",
        "state = next_state[0]  \n",
        "episode_reward += reward[0]  \n",
        "time += 1"
      ]
    }
  ]
}