{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras-0.2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPsp3wI+TqUAKRJ/I2xHBI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinhs0920/19-lab/blob/master/Keras_0_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHvKYUzRT-wn",
        "colab_type": "text"
      },
      "source": [
        "# 02. 심층 신경망 <br> -  ResNet[2][4] & DenseNet[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnvKGgxsT_v1",
        "colab_type": "text"
      },
      "source": [
        "## 함수형 API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqYWvks3Uc7-",
        "colab_type": "text"
      },
      "source": [
        "앞선 순차형 모델로는 구현할 수 없는 더 복잡한 신경망을 구축할 수 있는 도구이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAyKrV3MVjzV",
        "colab_type": "text"
      },
      "source": [
        "ex) 함수형 API에서 32개의 필터를 갖는 2차원 합성곱 계층을 Conv2D, 계층 입력텐서를 $x$, 계층 출력텐서를 $y$라 하면 아래와 같이 나타낸다.\n",
        "<br> $y=Conv2D(32)(x)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WjCnQZtUc-9",
        "colab_type": "text"
      },
      "source": [
        "다음 예시는 1장에서의 MNIST코드의 CNN을 함수형 API를 사용하여 나타낸 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6oppwbhSWNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b319f20-ac69-4995-dd8a-bebb22bc4013"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# MNIST dataset을 불러옵니다.\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 레이블 개수 계산\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# 원-핫 벡터로 변환\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# 입력 이미지 reshape 및 정규화\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# 신경망 매개변수\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 128\n",
        "kernel_size = 3\n",
        "filters = 64\n",
        "dropout = 0.3\n",
        "\n",
        "# 함수형 API를 사용해 CNN 계층 구축\n",
        "inputs = Input(shape=input_shape)\n",
        "y = Conv2D(filters=filters,\n",
        "           kernel_size=kernel_size,\n",
        "           activation='relu')(inputs)\n",
        "y = MaxPooling2D()(y)\n",
        "y = Conv2D(filters=filters,\n",
        "           kernel_size=kernel_size,\n",
        "           activation='relu')(y)\n",
        "y = MaxPooling2D()(y)\n",
        "y = Conv2D(filters=filters,\n",
        "           kernel_size=kernel_size,\n",
        "           activation='relu')(y)\n",
        "\n",
        "# dense 계층에 연결하기 전 이미지를 알맞은 벡터로 변환\n",
        "y = Flatten()(y)\n",
        "\n",
        "# dropout 정규화\n",
        "y = Dropout(dropout)(y)\n",
        "outputs = Dense(num_labels, activation='softmax')(y)\n",
        "\n",
        "# 입력/출력을 제공해 모델 구축\n",
        "model = Model(inputs=inputs, outputs=outputs) # Model() -> 텐서 리스트를 제공해주는 메서드 \n",
        "\n",
        "# 텍스트로 신경망 모델 요약\n",
        "model.summary()\n",
        "\n",
        "# 분류 모델 손실 함수, Adam 최적화, 정확도\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 입력 이미지와 레이블로 모델 훈련\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          validation_data=(x_test, y_test), # validation_data -> 모델 훈련동안 검증 정확도의 변화 확인을 위한 인수\n",
        "          epochs=20,\n",
        "          batch_size=batch_size)\n",
        "\n",
        "# 테스트 데이터세트에 대한 모델 정확도 평가\n",
        "score = model.evaluate(x_test,\n",
        "                       y_test,\n",
        "                       batch_size=batch_size,\n",
        "                       verbose=0)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5770      \n",
            "=================================================================\n",
            "Total params: 80,266\n",
            "Trainable params: 80,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.2766 - acc: 0.9148 - val_loss: 0.0621 - val_acc: 0.9800\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0723 - acc: 0.9774 - val_loss: 0.0396 - val_acc: 0.9864\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0495 - acc: 0.9845 - val_loss: 0.0304 - val_acc: 0.9901\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0412 - acc: 0.9868 - val_loss: 0.0272 - val_acc: 0.9914\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0343 - acc: 0.9894 - val_loss: 0.0232 - val_acc: 0.9929\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0305 - acc: 0.9898 - val_loss: 0.0241 - val_acc: 0.9922\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0262 - acc: 0.9917 - val_loss: 0.0250 - val_acc: 0.9919\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0209 - val_acc: 0.9931\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0216 - acc: 0.9930 - val_loss: 0.0235 - val_acc: 0.9918\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0199 - acc: 0.9936 - val_loss: 0.0213 - val_acc: 0.9937\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0174 - acc: 0.9942 - val_loss: 0.0267 - val_acc: 0.9912\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0153 - acc: 0.9952 - val_loss: 0.0264 - val_acc: 0.9922\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0152 - acc: 0.9948 - val_loss: 0.0197 - val_acc: 0.9942\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0141 - acc: 0.9953 - val_loss: 0.0212 - val_acc: 0.9928\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0128 - acc: 0.9956 - val_loss: 0.0232 - val_acc: 0.9938\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0120 - acc: 0.9960 - val_loss: 0.0232 - val_acc: 0.9933\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0111 - acc: 0.9960 - val_loss: 0.0269 - val_acc: 0.9921\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0110 - acc: 0.9964 - val_loss: 0.0287 - val_acc: 0.9920\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0086 - acc: 0.9973 - val_loss: 0.0265 - val_acc: 0.9936\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0276 - val_acc: 0.9935\n",
            "\n",
            "Test accuracy: 99.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK1C5sc-Zi2N",
        "colab_type": "text"
      },
      "source": [
        "1장에서 순차적 API를 사용했을 때의 정확도 99.4%와 거의 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO04k6SbZohu",
        "colab_type": "text"
      },
      "source": [
        "##입력이 두 개, 출력이 하나인 모델 생성하기 -> Y-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGE-T2iZokg",
        "colab_type": "text"
      },
      "source": [
        "Y-Network는 동일한 입력을 CNN의 왼쪽과 오른쪽 가지에 두번 사용한다. 이를 concatenate 계층을 사용해 결과를 결합한다.\n",
        "\n",
        "<br>예를 들면 (3,3,16)인 두 개의 텐서를 연결하면 결과로 형상이 (3,3,32)인 텐서를 얻게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU9BX0CBZonf",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73603935-c93bb600-45cc-11ea-93e1-91734c10d600.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjYCF8R5Zorg",
        "colab_type": "text"
      },
      "source": [
        "앞선 코드 모델의 성능을 개선하기 위해 다음 작업을 수행한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IX1jmgiZouG",
        "colab_type": "text"
      },
      "source": [
        "1.Y-Network의 가지에서 필터 수를 두 배로 늘려 MaxPooling2D() 다음에서 특징 맵 크기가 절반으로 줄어든 것을 보완한다. 예를 들어, 첫 번째 합성곱의 출력이 (28,28,32)이면 맥스 풀링 이후에 형상이 (14,14,32)가 된다. 그러면 다음 합성곱 계층의 필터 크기는 64이고 출력은 (14,14,64)이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaJsOfE_ZoxS",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604002-a3fb7780-45cd-11ea-88db-60b991a683f5.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c54sg0F_Zo0I",
        "colab_type": "text"
      },
      "source": [
        "2.오른쪽 한쪽 가지에서 팽창률(dilation rate)을 2로 적용한다. 팽창률을 사용해 커널의 적용 범위를 증가시키면 오른쪽 가지가 더 다양한 특징맵을 학습시킬 수 있다는 뜻이다. padding='same' 을 사용해 팽창된 CNN의 텐서 차원이 음수가 되지않게하고 입력 차원을 출력인 특징 맵과 동일하게 유지한다. 출력과 같은 크기를 유지하려고 입력의 나머지 부분을 0으로 채워 패딩한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsCMbbqvhDfd",
        "colab_type": "text"
      },
      "source": [
        "다음 코드는 함수형 API를 사용해 Y-Network를 구현하는 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xk-Rmq8hIV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "783ef113-5e4f-4b11-984f-b6c0c6d3455d"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten, concatenate # Y-Network의 특징인 concatenate를 사용하였다.\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 32\n",
        "kernel_size = 3\n",
        "dropout = 0.4\n",
        "n_filters = 32\n",
        "\n",
        "#  Y-Network의 왼쪽 가지\n",
        "left_inputs = Input(shape=input_shape)\n",
        "x = left_inputs\n",
        "filters = n_filters\n",
        "# Conv2D-Dropout-MaxPooling2D 3개의 층으로 구현\n",
        "# 계층이 지날 때 마다 필터 개수를 2배 증가 (32-64-128)\n",
        "for i in range(3):\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               padding='same',\n",
        "               activation='relu')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    filters *= 2\n",
        "\n",
        "# Y-Network의 오른쪽 가지\n",
        "right_inputs = Input(shape=input_shape)\n",
        "y = right_inputs\n",
        "filters = n_filters\n",
        "# Conv2D-Dropout-MaxPooling2D 3개층 구성\n",
        "# 계층이 지날 때 마다 필터 개수를 2배 증가 (32-64-128)\n",
        "for i in range(3):\n",
        "    y = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               padding='same',\n",
        "               activation='relu',\n",
        "               dilation_rate=2)(y)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = MaxPooling2D()(y)\n",
        "    filters *= 2\n",
        "\n",
        "# 왼쪽과 오른쪽 가지 출력 병합\n",
        "y = concatenate([x, y])\n",
        "\n",
        "# Dense 계층에 연결하기 전 특징 맵을 벡터로 변환 \n",
        "y = Flatten()(y)\n",
        "y = Dropout(dropout)(y)\n",
        "outputs = Dense(num_labels, activation='softmax')(y)\n",
        "\n",
        "# 함수형 API에서 모델 구축\n",
        "model = Model([left_inputs, right_inputs], outputs)\n",
        "\n",
        "# 그래프를 사용해 모델 확인\n",
        "plot_model(model, to_file='cnn-y-network.png', show_shapes=True)\n",
        "\n",
        "# 계층 텍스트 설명을 사용해 모델 확인\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit([x_train, x_train],\n",
        "          y_train, \n",
        "          validation_data=([x_test, x_test], y_test),\n",
        "          epochs=20,\n",
        "          batch_size=batch_size)\n",
        "\n",
        "score = model.evaluate([x_test, x_test],\n",
        "                       y_test,\n",
        "                       batch_size=batch_size,\n",
        "                       verbose=0)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 28, 28, 32)   320         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 28, 28, 32)   320         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 28, 28, 32)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 28, 28, 32)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 32)   0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 32)   0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 14, 14, 64)   18496       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 14, 14, 64)   18496       max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 14, 14, 64)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 14, 14, 64)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 7, 7, 64)     0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 7, 7, 64)     0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 7, 7, 128)    73856       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 7, 7, 128)    73856       max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 7, 7, 128)    0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 7, 7, 128)    0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 3, 3, 128)    0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 3, 3, 128)    0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3, 3, 256)    0           max_pooling2d_6[0][0]            \n",
            "                                                                 max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 2304)         0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           23050       dropout_8[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 208,394\n",
            "Trainable params: 208,394\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 14s 241us/sample - loss: 0.1743 - acc: 0.9454 - val_loss: 0.1375 - val_acc: 0.9882\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 14s 229us/sample - loss: 0.0665 - acc: 0.9793 - val_loss: 0.0961 - val_acc: 0.9898\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0535 - acc: 0.9832 - val_loss: 0.1078 - val_acc: 0.9904\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0482 - acc: 0.9851 - val_loss: 0.0673 - val_acc: 0.9911\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0433 - acc: 0.9859 - val_loss: 0.0618 - val_acc: 0.9925\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 14s 231us/sample - loss: 0.0393 - acc: 0.9876 - val_loss: 0.0604 - val_acc: 0.9917\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0388 - acc: 0.9880 - val_loss: 0.0467 - val_acc: 0.9924\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0372 - acc: 0.9882 - val_loss: 0.0706 - val_acc: 0.9935\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0343 - acc: 0.9892 - val_loss: 0.0473 - val_acc: 0.9930\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0345 - acc: 0.9892 - val_loss: 0.0391 - val_acc: 0.9942\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0329 - acc: 0.9898 - val_loss: 0.0584 - val_acc: 0.9931\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 14s 227us/sample - loss: 0.0345 - acc: 0.9893 - val_loss: 0.0385 - val_acc: 0.9939\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 13s 217us/sample - loss: 0.0304 - acc: 0.9902 - val_loss: 0.0354 - val_acc: 0.9941\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 13s 218us/sample - loss: 0.0316 - acc: 0.9900 - val_loss: 0.0308 - val_acc: 0.9931\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 13s 220us/sample - loss: 0.0316 - acc: 0.9903 - val_loss: 0.0620 - val_acc: 0.9934\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 13s 220us/sample - loss: 0.0297 - acc: 0.9909 - val_loss: 0.0332 - val_acc: 0.9938\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 13s 220us/sample - loss: 0.0316 - acc: 0.9905 - val_loss: 0.0315 - val_acc: 0.9949\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 13s 219us/sample - loss: 0.0302 - acc: 0.9905 - val_loss: 0.0443 - val_acc: 0.9931\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 13s 218us/sample - loss: 0.0318 - acc: 0.9905 - val_loss: 0.0392 - val_acc: 0.9918\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 13s 217us/sample - loss: 0.0299 - acc: 0.9909 - val_loss: 0.0378 - val_acc: 0.9909\n",
            "\n",
            "Test accuracy: 99.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axpfHqEMiqNk",
        "colab_type": "text"
      },
      "source": [
        "또 하나의 특징은 Y-Network에서는 훈련과 검증에 두 개의 입력이 필요하므로, [x-train, x-train]이 제공된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baQIm-S5jo1m",
        "colab_type": "text"
      },
      "source": [
        "아래 그림은  위 코드를 구현한 CNN MNIST Y-Network이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHFcus71jUem",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604144-5849cd80-45cf-11ea-8dba-f183d6041867.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4hY67fDj1pd",
        "colab_type": "text"
      },
      "source": [
        "# 심층 잔차 신경망(ResNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgaTzXPDj10n",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604184-cee6cb00-45cf-11ea-9869-acd291c41730.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ8m33CMj1sk",
        "colab_type": "text"
      },
      "source": [
        "심층 신경망에서 문제는 역전파가 이루어지면서 그레디언트 손실이 발생하는 것이다.\n",
        "<br>역전파는 연쇄법칙을 기반으로 하며 그레디언트는 입력층으로 갈수록 손실이 진행된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxOoenCkj1xt",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604231-677d4b00-45d0-11ea-8ef7-21f17cd90f94.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TfuwDXBj13k",
        "colab_type": "text"
      },
      "source": [
        "  ResNet 블록은 일반 CNN 블록과 비교했을 때 역전파 중 그레디언트 손실을 막기위해 **숏컷 연결**을 도입하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuzJ2ofTl_21",
        "colab_type": "text"
      },
      "source": [
        "### CNN과 ResNet 차이점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQGDxfAJj16W",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604261-eecabe80-45d0-11ea-9d06-8a3f022b1280.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgsAMy2_l_7y",
        "colab_type": "text"
      },
      "source": [
        "##### CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFBWiYrMl__D",
        "colab_type": "text"
      },
      "source": [
        "CNN 계층의 연산은 Conv2D - 배치 정규화- ReLU 를 통하여 진행된다. \n",
        "\n",
        "<br>즉 특정 계층 l의 출력 특징 맵은 이전 특징 맵에서만 직접적으로 영향받는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY7K7_dfmACY",
        "colab_type": "text"
      },
      "source": [
        "##### ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R04HCcqmAFf",
        "colab_type": "text"
      },
      "source": [
        "ResNet 계층 연산은  Conv2D - 배치 정규화- ReLU 연산을 진행한 출력이  Conv2D - 배치 정규화 과정의 연산을 한번 더 진행한 출력과 원래 입력값을 더하여 ReLU 함수 연산을 진행하여 출력을 낸다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMucUqRkmAIm",
        "colab_type": "text"
      },
      "source": [
        "이에 대한 예제로는 CIFAR10을 활용할 것이다.\n",
        "<br> CIFAR10 역시 MNIST처럼 10개의 카테고리로 되어 있다. (32X32)의 RGB로 되어있다. 5만개의 레이블이 달린 훈련 이미지와, 1만개의 검증 이미지로 구성되어 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9aLkE_jqPrc",
        "colab_type": "text"
      },
      "source": [
        "아래 그림은 CIFAR10 데이터세트에 대한 ResNet 모델 아키텍처를 보여준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S50OUOFHpRhN",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604362-eecbbe00-45d2-11ea-9d0b-cd615276b38a.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0gJ3WXYpRk8",
        "colab_type": "text"
      },
      "source": [
        "아래 표는 ResNet 아키텍쳐 구성이다.\n",
        "![image](https://user-images.githubusercontent.com/53015968/73604469-b5944d80-45d4-11ea-8301-6d74a02f3b30.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFamkg1opRpS",
        "colab_type": "text"
      },
      "source": [
        "이는 잔차 블록 집합이 3개 있다는 뜻이며, 각 집합에는 n개의 잔차 블록이 있다면 2n개의 계층이 있다.\n",
        "\n",
        "<br> 서로 다른 크기의 두 특징 맵 사이의 전이를 제외하면 커널 크기는 전부 3 이다.전이 계층에서는 커널 크기가 1이고 strides=2 일때의 Conv2D이다.\n",
        "\n",
        "*) strides=2 뜻은 합성곱 적용 시, 픽셀을 하나씩 건너뛰면서 커널을 적용\n",
        "\n",
        "<BR> 서로 다른 크기의 두 잔차 블록을 연결할 때 전이 계층을 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKC97Z2UpRtJ",
        "colab_type": "text"
      },
      "source": [
        "마지막 계층은 AveragePooling2D-Faltten-Dense로 구성된다. ResNet은 dropout을 사용하지 않는다. 또한, 덧셈 연산과 1X1 합성곱 적용은 자신을 정규화하는 효과를 가진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwV5B_k6pRwe",
        "colab_type": "text"
      },
      "source": [
        "CIFAR10 데이터세트 분류를 위한 ResNet 아키텍쳐\n",
        "![image](https://user-images.githubusercontent.com/53015968/73604527-b1b4fb00-45d5-11ea-99c5-b54ee8b50375.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWuLuAVAuKzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59035d1a-bb93-44d3-b108-d393196e65b1"
      },
      "source": [
        "# ResNet v1과 ResNet v2가 모두 구현되어 있다.\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input\n",
        "from tensorflow.keras.layers import Flatten, add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 훈련 매개변수\n",
        "batch_size = 32\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# 모델 파라미터\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# 모델 버전\n",
        "# 최초 논문: version = 1 (ResNet v1), \n",
        "version = 1\n",
        "\n",
        "# 제공된 모델 매개변수 n으로부터 계산된 네트워크 깊이\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# CIFAR10 데이터 불러오기\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# 정규화\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# if subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\" 학습률은 lr_schedule()을 통해 80, 120, 160, 180 epochs 에서 시작하여 점차 감소시킨다.\n",
        "    기본값은 1e-3 이다. lr_schedule()함수는 모델 훈련동안 한 epoch 당 callbacks 변수로 호출된다.\n",
        "\n",
        "    # Arguments\n",
        "        에폭의 수 :-> epoch (int)\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "    Arguments:\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "    Returns:\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal', # 역전파 시 수렴될 수 있게 해주는 것\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, in [a])')\n",
        "    \n",
        "    # 모델 정의 시작\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    \n",
        "    # 잔차 유닛 인스턴스화\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "           \n",
        "            if stack > 0 and res_block == 0:  \n",
        "                strides = 2  \n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            \n",
        "            if stack > 0 and res_block == 0:\n",
        "               \n",
        "               # 변경된 차원을 맞추기 위해 잔차 숏컷 연결을 선형으로 사영 \n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # 분류기 추가\n",
        "    # v1은 마지막 숏컷 연결-ReLU 후에는 BN을 사용하지 않는다.\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # 모델 인스턴스화\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "#ResNet v2 구현\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "   \n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 110 in [b])')\n",
        "   \n",
        "   # 모델 정의\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "   # v2에서는 2 경로로 나뉘기 전에 입력에 BN-ReLU와 함께 Conv2D를 수행 \n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    \n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                # first layer and first stage\n",
        "                if res_block == 0:  \n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "               \n",
        "                if res_block == 0:\n",
        "                   \n",
        "                    strides = 2 \n",
        "\n",
        "           \n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "              \n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "plot_model(model, to_file=\"%s.png\" % model_type, show_shapes=True)\n",
        "print(model_type)\n",
        "\n",
        "# prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # this will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False)\n",
        "\n",
        "    # compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, \n",
        "                        steps_per_epoch=len(x_train)//batch_size,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# score trained model\n",
        "scores = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Learning rate:  0.001\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 32)   4640        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 32)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 32)   9248        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 32)   544         activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 32)   0           conv2d_9[0][0]                   \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 32)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 32)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 32)   0           activation_8[0][0]               \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 32)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 32)   0           activation_10[0][0]              \n",
            "                                                                 batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 64)     18496       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 64)     36928       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 64)     2112        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 64)     0           conv2d_16[0][0]                  \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 64)     0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 64)     0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 64)     0           activation_14[0][0]              \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 64)     0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 64)     0           activation_16[0][0]              \n",
            "                                                                 batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n",
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlYWCdJTpRz6",
        "colab_type": "text"
      },
      "source": [
        "## ResNet v2 - 개선된 ResNet 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZkQaT7CpR3D",
        "colab_type": "text"
      },
      "source": [
        "### ResNet v1 과 ResNet v2 비교"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tatb2wSc4ii9",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605043-8f72ab80-45dc-11ea-8a66-530581ea7d76.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6nQYlZl5DYs",
        "colab_type": "text"
      },
      "source": [
        "* 연산량을 줄이기 위해 1X1-3X3-1X1 BN=ReLU-Conv2D 스택을 사용 - 병목 레이어 (bottleneck layer)\n",
        "* 배치 정규화와 ReLU 활성화 함수를 2D 합성곱 전에 위치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBIWENKX5Ddg",
        "colab_type": "text"
      },
      "source": [
        "## 밀집 연결 합성곱 네트워크(DenseNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up1GS3AZ651F",
        "colab_type": "text"
      },
      "source": [
        "Dense는 R$esNet과 달리 숏컷 연결이 아닌 이전 특징 맵 전체가 다음 계층의 입력이 되는 방식으로 그레디언트 소실문제를 해결한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPg9cni68vfn",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605264-833c1d80-45df-11ea-976d-d6d88cb731fd.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWLwBIJe8vnM",
        "colab_type": "text"
      },
      "source": [
        "Conv2D 역시 크기가 3인 커널을 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HlCadKg-2R9",
        "colab_type": "text"
      },
      "source": [
        "계층마다 생성되는 특징 맵의 개수를 성장률$k$라고 한다.\n",
        "<br>일반적으로 $k$=12를 사용한다. 특정 맵 $x_0$ 개수가 $k_0$일 때 위 그림처럼 4개의 계층으로 구성된 밀집 계층의 끝에서 특징 맵의 전체 개수는 $4 \\times k +k_0 $이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOMrUSLFAFBE",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605405-2b9eb180-45e1-11ea-9d25-a247de91a561.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeHCfh0VAmfo",
        "colab_type": "text"
      },
      "source": [
        "병합 연산에서 연결(Concat)을 사용하기 위해 크기의 차이를 일치하여야 한다. 특징 맵 개수가 효율적인 개수까지 증가하도록 하기위해 우측 그림처럼 병목 계층을 도입했다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8GV4qVqBXjM",
        "colab_type": "text"
      },
      "source": [
        "연결 후에는 필터 크기가 $4k$인 $1 \\times 1$ 합성곱이 적용된다.\n",
        "<br> 이 것은 Conv2D(3)에서 처리되는 특징 맵 개수가 빠르게 증가하는 것을 막는다.\n",
        "\n",
        "<br> 병목 도입을 안할 떈 Conv2D(3)에서 받는 입력은 1224개고\n",
        "병목계층을 도입한 후에는 48개의 특징 맵만 입력받는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9wM2aNYBm9C",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605482-33128a80-45e2-11ea-9290-261585335e8e.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCezasybCSRz",
        "colab_type": "text"
      },
      "source": [
        "특징 맵 크기는 서로 다 다르기 때문에 이를 해결하기 위해 DenseNet은 심층 신경망을 여러 개의 밀집 블록으로 나누고 전이 계층을 통해 서로 연결시킨다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m28NMnC1CbLk",
        "colab_type": "text"
      },
      "source": [
        "각 밀집 블록 내에서는 특징 맵 크기(너비, 높이)가 일정하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXmMi62dCgRE",
        "colab_type": "text"
      },
      "source": [
        "전이 계층 : 두 밀집 블록 사이에서 한 특징 맵 크기에서 더 작은 특징 맵 크기로 바꾸는 것 (일반적으론 절반으로 줄인다)\n",
        "\n",
        "<br> 위 기능은 AveragePooling2D 계층이 수행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF0BcDWYC0dB",
        "colab_type": "text"
      },
      "source": [
        "예를 들면, 기본 pool_size=2 를 적용한 AveragePooling2D를 사용하면 맵 크기가 (64,64,256)에서 (32,32,256)으로 줄어든다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHcp_1HnDEek",
        "colab_type": "text"
      },
      "source": [
        "하지만, 특징 맵이 AveragePooling2D 으로 전달되기 전에 Conv2D(1)을 사용해 맵 개수를 특정 압축비 $0<\\theta<1$로 줄인다. 여기선 0.5로 정한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toGhv60mC0gT",
        "colab_type": "text"
      },
      "source": [
        "따라서 (64,64,512)의 출력이 Conv2D에서 (64,64,256)으로 줄어들고, AveragePooling2D를 통해 (32,32,256)으로 한번더 줄어드는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHR2_0_eDtnk",
        "colab_type": "text"
      },
      "source": [
        "### CIFAR10을 위한 100계층 DenseNet-BC 구성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWcfeFBkD0lc",
        "colab_type": "text"
      },
      "source": [
        "이제 CIFAR10 데이터 세트에 100개의 계층으로 구성된 DenseNet-BC(병목 압축)을 구성한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4HriTX3D_P6",
        "colab_type": "text"
      },
      "source": [
        "아래 표는 100개의 계층으로 이루어진 DenseNet-BC이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFfgeKJpEJeE",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605581-6275c700-45e3-11ea-8ba9-256081c85a5c.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16xwX57-EJiR",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605596-93ee9280-45e3-11ea-8c40-9d071d9cf6fb.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gm_f-SLEKFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Input, Flatten, Dropout\n",
        "from tensorflow.keras.layers import concatenate, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# 훈련 매개변수\n",
        "batch_size = 32\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "\n",
        "# 네트워크 매개변수\n",
        "num_classes = 10\n",
        "num_dense_blocks = 3\n",
        "use_max_pool = False\n",
        "\n",
        "# DenseNet-BC 데이터 셋\n",
        "# Growth rate   | Depth |  Accuracy (paper)| Accuracy (this)      |\n",
        "# 12            | 100   |  95.49%          | 93.74%               |\n",
        "# 24            | 250   |  96.38%          | requires big mem GPU |\n",
        "# 40            | 190   |  96.54%          | requires big mem GPU |\n",
        "growth_rate = 12\n",
        "depth = 100\n",
        "num_bottleneck_layers = (depth - 4) // (2 * num_dense_blocks)\n",
        "\n",
        "num_filters_bef_dense_block = 2 * growth_rate\n",
        "compression_factor = 0.5\n",
        "\n",
        "# CIFAR10 데이터 불러오기\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# 데이터 정규화\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "# 모델 정의\n",
        "# densenet CNNs (합성함수)는 BN-ReLU-Conv2D로 구성됨\n",
        "inputs = Input(shape=input_shape)\n",
        "x = BatchNormalization()(inputs)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(num_filters_bef_dense_block,\n",
        "           kernel_size=3,\n",
        "           padding='same',\n",
        "           kernel_initializer='he_normal')(x)\n",
        "x = concatenate([inputs, x])\n",
        "\n",
        "# 전이층으로 연결된 밀집 블록의 스택\n",
        "for i in range(num_dense_blocks):\n",
        "    # 밀집 블록은 병목 계층의 스택이다\n",
        "    for j in range(num_bottleneck_layers):\n",
        "        y = BatchNormalization()(x)\n",
        "        y = Activation('relu')(y)\n",
        "        y = Conv2D(4 * growth_rate,\n",
        "                   kernel_size=1,\n",
        "                   padding='same',\n",
        "                   kernel_initializer='he_normal')(y)\n",
        "        if not data_augmentation:\n",
        "            y = Dropout(0.2)(y)\n",
        "        y = BatchNormalization()(y)\n",
        "        y = Activation('relu')(y)\n",
        "        y = Conv2D(growth_rate,\n",
        "                   kernel_size=3,\n",
        "                   padding='same',\n",
        "                   kernel_initializer='he_normal')(y)\n",
        "        if not data_augmentation:\n",
        "            y = Dropout(0.2)(y)\n",
        "        x = concatenate([x, y])\n",
        "\n",
        "    # 마미작 밀집 블록 다음에는 전이 계층이 없다\n",
        "    if i == num_dense_blocks - 1:\n",
        "        continue\n",
        "\n",
        "    # 전이 계층에서 특징 맵 개수를 압축하고 크기를 1/2로 줄인다.\n",
        "    num_filters_bef_dense_block += num_bottleneck_layers * growth_rate\n",
        "    num_filters_bef_dense_block = int(num_filters_bef_dense_block * compression_factor)\n",
        "    y = BatchNormalization()(x)\n",
        "    y = Conv2D(num_filters_bef_dense_block,\n",
        "               kernel_size=1,\n",
        "               padding='same',\n",
        "               kernel_initializer='he_normal')(y)\n",
        "    if not data_augmentation:\n",
        "        y = Dropout(0.2)(y)\n",
        "    x = AveragePooling2D()(y)\n",
        "\n",
        "\n",
        "# 애버리지 풀링 다음으로 상단에 분류 모델 추가\n",
        "# 특징 맵 크기는1 x 1\n",
        "x = AveragePooling2D(pool_size=8)(x)\n",
        "y = Flatten()(x)\n",
        "outputs = Dense(num_classes,\n",
        "                kernel_initializer='he_normal',\n",
        "                activation='softmax')(y)\n",
        "\n",
        "# 최초 논문에서는 SGD를 사용했지만 DenseNet에서는 RMSprop 가 성능이 좋다.\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(1e-3),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "plot_model(model, to_file=\"cifar10-densenet.png\", show_shapes=True)\n",
        "\n",
        "# prepare model model saving directory\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_densenet_model.{epoch:02d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# prepare callbacks for model saving and for learning rate reducer\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# run training, with or without data augmentation\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # preprocessing  and realtime data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (deg 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally\n",
        "        height_shift_range=0.1,  # randomly shift images vertically\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "    # compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied)\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # fit the model on the batches generated by datagen.flow()\n",
        "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# score trained model\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}