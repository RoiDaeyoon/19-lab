{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL PPO.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM69k0EQpYxs8Zufjh1oSUa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianshin12/19-lab/blob/master/RL_PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iAvqrcydR1O",
        "colab_type": "text"
      },
      "source": [
        "PPO 근접 정책 최적화  \n",
        "정책 그래디언트 이용  \n",
        "점진적으로 업테이트할 수 있는 방법  \n",
        "온-폴리시 방법  \n",
        "정책을 한 단계 업데이트한 후 이전 정책이 발생시긴 샘플 폐기 후 업데이트된 정책으로 샘플 발생 -> 반복 = 효율성이 떨어짐  \n",
        "-> 오프-폴리시=>다른 정책으로 발생시킨 샘플로도 해당 정책 업데이트 가능  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utQ59EtbwyZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JyUN9RA2hUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install --upgrade tensorflow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnxSTNVLc9y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Lambda\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kmeW7Icf2jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PPO with GAE Actor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda\n",
        "\n",
        "class Actor(object):\n",
        "    \"\"\"\n",
        "        Actor Network for PPO\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, action_bound, learning_rate, ratio_clipping):\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_bound = action_bound\n",
        "        self.learning_rate = learning_rate\n",
        "        self.ratio_clipping = ratio_clipping\n",
        "\n",
        "        self.std_bound = [1e-2, 1.0] # std bound\n",
        "\n",
        "        self.model = self.build_network()\n",
        "\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
        "\n",
        "    ## actor network\n",
        "    def build_network(self):\n",
        "        state_input = Input((self.state_dim,))\n",
        "        h1 = Dense(64, activation='relu')(state_input)\n",
        "        h2 = Dense(32, activation='relu')(h1)\n",
        "        h3 = Dense(16, activation='relu')(h2)\n",
        "        out_mu = Dense(self.action_dim, activation='tanh')(h3)\n",
        "        std_output = Dense(self.action_dim, activation='softplus')(h3)\n",
        "\n",
        "        # Scale output to [-action_bound, action_bound]\n",
        "        mu_output = Lambda(lambda x: x*self.action_bound)(out_mu)\n",
        "        model = Model(state_input, [mu_output, std_output])\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "\n",
        "    ## log policy pdf\n",
        "    def log_pdf(self, mu, std, action):\n",
        "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
        "        var = std**2\n",
        "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
        "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
        "\n",
        "\n",
        "    ## actor policy\n",
        "    def get_policy_action(self, state):\n",
        "        # type of action in env is numpy array\n",
        "        # np.reshape(state, [1, self.state_dim]) : shape (state_dim,) -> shape (1, state_dim)\n",
        "        # why [0]?  shape (1, action_dim) -> (action_dim,)\n",
        "        mu_a, std_a = self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
        "        mu_a = mu_a[0]\n",
        "        std_a = std_a[0]\n",
        "        std_a = np.clip(std_a, self.std_bound[0], self.std_bound[1])\n",
        "        action = np.random.normal(mu_a, std_a, size=self.action_dim)\n",
        "        return mu_a, std_a, action\n",
        "\n",
        "    ## actor prediction\n",
        "    def predict(self, state):\n",
        "        mu_a, _= self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
        "        return mu_a[0]\n",
        "\n",
        "\n",
        "    ## train the actor network\n",
        "    def train(self, log_old_policy_pdf, states, actions, advantages):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # current policy pdf\n",
        "            mu_a, std_a = self.model(states)\n",
        "            log_policy_pdf = self.log_pdf(mu_a, std_a, actions)\n",
        "\n",
        "            # ratio of current and old policies\n",
        "            ratio = tf.exp(log_policy_pdf - log_old_policy_pdf)\n",
        "            clipped_ratio = tf.clip_by_value(ratio, 1.0-self.ratio_clipping, 1.0+self.ratio_clipping)\n",
        "            surrogate = -tf.minimum(ratio * advantages, clipped_ratio * advantages)\n",
        "            loss = tf.reduce_mean(surrogate)\n",
        "        dj_dtheta = tape.gradient(loss, self.model.trainable_variables)\n",
        "        grads = zip(dj_dtheta, self.model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(grads)\n",
        "\n",
        "    ## save actor weights\n",
        "    def save_weights(self, path):\n",
        "        self.model.save_weights(path)\n",
        "\n",
        "\n",
        "    ## load actor wieghts\n",
        "    def load_weights(self, path):\n",
        "        self.model.load_weights(path + 'pendulum_actor.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5woNqIknGqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PPO with GAE Agent for training and evaluation\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class PPOagent(object):\n",
        "\n",
        "    def __init__(self, env):\n",
        "\n",
        "        # hyperparameters\n",
        "        self.GAMMA = 0.95\n",
        "        self.GAE_LAMBDA = 0.9 #0.8\n",
        "        self.BATCH_SIZE = 64\n",
        "        self.ACTOR_LEARNING_RATE = 0.0001\n",
        "        self.CRITIC_LEARNING_RATE = 0.001\n",
        "        self.RATIO_CLIPPING = 0.2\n",
        "        self.EPOCHS = 10\n",
        "\n",
        "        self.env = env\n",
        "        # get state dimension\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        # get action dimension\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        # get action bound\n",
        "        self.action_bound = env.action_space.high[0]\n",
        "\n",
        "        # create actor and critic networks\n",
        "        self.actor = Actor(self.state_dim, self.action_dim, self.action_bound,\n",
        "                           self.ACTOR_LEARNING_RATE, self.RATIO_CLIPPING)\n",
        "        self.critic = Critic(self.state_dim, self.action_dim, self.CRITIC_LEARNING_RATE)\n",
        "\n",
        "        # save the results\n",
        "        self.save_epi_reward = []\n",
        "\n",
        "\n",
        "    ## computing Advantages and targets: y_k = r_k + gamma*V(s_k+1), A(s_k, a_k)= y_k - V(s_k)\n",
        "    def gae_target(self, rewards, v_values, next_v_value, done):\n",
        "        n_step_targets = np.zeros_like(rewards)\n",
        "        gae = np.zeros_like(rewards)\n",
        "        gae_cumulative = 0\n",
        "        forward_val = 0\n",
        "\n",
        "        if not done:\n",
        "            forward_val = next_v_value\n",
        "\n",
        "        for k in reversed(range(0, len(rewards))):\n",
        "            delta = rewards[k] + self.GAMMA * forward_val - v_values[k]\n",
        "            gae_cumulative = self.GAMMA * self.GAE_LAMBDA * gae_cumulative + delta\n",
        "            gae[k] = gae_cumulative\n",
        "            forward_val = v_values[k]\n",
        "            n_step_targets[k] = gae[k] + v_values[k]\n",
        "        return gae, n_step_targets\n",
        "\n",
        "\n",
        "    ## convert (list of np.array) to np.array\n",
        "    def unpack_batch(self, batch):\n",
        "        unpack = batch[0]\n",
        "        for idx in range(len(batch)-1):\n",
        "            unpack = np.append(unpack, batch[idx+1], axis=0)\n",
        "\n",
        "        return unpack\n",
        "\n",
        "\n",
        "    ## train the agent\n",
        "    def train(self, max_episode_num):\n",
        "\n",
        "        # initialize batch\n",
        "        batch_state, batch_action, batch_reward = [], [], []\n",
        "        batch_log_old_policy_pdf = []\n",
        "\n",
        "        for ep in range(int(max_episode_num)):\n",
        "\n",
        "            # reset episode\n",
        "            time, episode_reward, done = 0, 0, False\n",
        "            # reset the environment and observe the first state\n",
        "            state = self.env.reset() # shape of state from gym (3,)\n",
        "\n",
        "            while not done:\n",
        "\n",
        "                # visualize the environment\n",
        "                #self.env.render()\n",
        "                # compute mu and std of old policy and pick an action (shape of gym action = (action_dim,) )\n",
        "                mu_old, std_old, action = self.actor.get_policy_action(state)\n",
        "                # clip continuous action to be within action_bound\n",
        "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
        "                # compute log old policy pdf\n",
        "                var_old = std_old ** 2\n",
        "                log_old_policy_pdf = -0.5 * (action - mu_old) ** 2 / var_old - 0.5 * np.log(var_old * 2 * np.pi)\n",
        "                log_old_policy_pdf = np.sum(log_old_policy_pdf)\n",
        "                # observe reward, new_state, shape of output of gym (state_dim,)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                # change shape (state_dim,) -> (1, state_dim), same to mu, std, action\n",
        "                state = np.reshape(state, [1, self.state_dim])\n",
        "                action = np.reshape(action, [1, self.action_dim])\n",
        "                reward = np.reshape(reward, [1, 1])\n",
        "                log_old_policy_pdf = np.reshape(log_old_policy_pdf, [1, 1])\n",
        "\n",
        "                # append to the batch\n",
        "                batch_state.append(state)\n",
        "                batch_action.append(action)\n",
        "                batch_reward.append((reward+8)/8) # <-- normalization\n",
        "                #batch_reward.append(reward)\n",
        "                batch_log_old_policy_pdf.append(log_old_policy_pdf)\n",
        "\n",
        "                # continue until batch becomes full\n",
        "                if len(batch_state) < self.BATCH_SIZE:\n",
        "                    # update current state\n",
        "                    state = next_state\n",
        "                    episode_reward += reward[0]\n",
        "                    time += 1\n",
        "                    continue\n",
        "\n",
        "                # extract batched states, actions, td_targets, advantages\n",
        "                states = self.unpack_batch(batch_state)\n",
        "                actions = self.unpack_batch(batch_action)\n",
        "                rewards = self.unpack_batch(batch_reward)\n",
        "                log_old_policy_pdfs = self.unpack_batch(batch_log_old_policy_pdf)\n",
        "\n",
        "                # clear the batch\n",
        "                batch_state, batch_action, batch_reward = [], [], []\n",
        "                batch_log_old_policy_pdf = []\n",
        "\n",
        "                # compute gae and TD targets\n",
        "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
        "                next_v_value = self.critic.model.predict(next_state)\n",
        "                v_values = self.critic.model.predict(states)\n",
        "                gaes, y_i = self.gae_target(rewards, v_values, next_v_value, done)\n",
        "\n",
        "                # update the networks\n",
        "                for _ in range(self.EPOCHS):\n",
        "\n",
        "                    # train\n",
        "                    self.actor.train(log_old_policy_pdfs, states, actions, gaes)\n",
        "                    self.critic.train_on_batch(states, y_i)\n",
        "\n",
        "                # update current state\n",
        "                state = next_state\n",
        "                episode_reward += reward[0]\n",
        "                time += 1\n",
        "\n",
        "            ## display rewards every episode\n",
        "            print('Episode: ', ep+1, 'Time: ', time, 'Reward: ', episode_reward)\n",
        "\n",
        "            self.save_epi_reward.append(episode_reward)\n",
        "\n",
        "\n",
        "            ## save weights every episode\n",
        "            if ep % 10 == 0:\n",
        "                self.actor.save_weights(\"./save_weights/pendulum_actor.h5\")\n",
        "                self.critic.save_weights(\"./save_weights/pendulum_critic.h5\")\n",
        "\n",
        "        np.savetxt('./save_weights/pendulum_epi_reward.txt', self.save_epi_reward)\n",
        "        print(self.save_epi_reward)\n",
        "\n",
        "\n",
        "    ## save them to file if done\n",
        "    def plot_result(self):\n",
        "        plt.plot(self.save_epi_reward)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4y_rzr2hAhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PPO with GAE Critic\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class Critic(object):\n",
        "    \"\"\"\n",
        "        Critic Network for PPO: V function approximator\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, learning_rate):\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # create and compile critic network\n",
        "        self.model, self.states = self.build_network()\n",
        "\n",
        "        self.model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
        "\n",
        "    ## critic network\n",
        "    def build_network(self):\n",
        "        state_input = Input((self.state_dim,))\n",
        "        h1 = Dense(64, activation='relu')(state_input)\n",
        "        h2 = Dense(32, activation='relu')(h1)\n",
        "        h3 = Dense(16, activation='relu')(h2)\n",
        "        v_output = Dense(1, activation='linear')(h3)\n",
        "        model = Model(state_input, v_output)\n",
        "        model.summary()\n",
        "        return model, state_input\n",
        "\n",
        "\n",
        "    ## single gradient update on a single batch data\n",
        "    def train_on_batch(self, states, td_targets):\n",
        "        return self.model.train_on_batch(states, td_targets)\n",
        "\n",
        "\n",
        "    ## save critic weights\n",
        "    def save_weights(self, path):\n",
        "        self.model.save_weights(path)\n",
        "\n",
        "\n",
        "    ## load critic wieghts\n",
        "    def load_weights(self, path):\n",
        "        self.model.load_weights(path + 'pendulum_critic.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3SB0paGmrKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PPO with GAE main\n",
        "\n",
        "import gym\n",
        "\n",
        "def main():\n",
        "\n",
        "    max_episode_num = 1000\n",
        "    env_name = 'Pendulum-v0'\n",
        "    env = gym.make(env_name)\n",
        "    agent = PPOagent(env)\n",
        "\n",
        "    agent.train(max_episode_num)\n",
        "\n",
        "    agent.plot_result()\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ewv2Of3mwi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PPO with GAE Load and Play\n",
        "\n",
        "import gym\n",
        "\n",
        "def main():\n",
        "\n",
        "    env_name = 'Pendulum-v0'\n",
        "    env = gym.make(env_name)\n",
        "    agent = PPOagent(env)\n",
        "\n",
        "    agent.actor.load_weights('./save_weights/')\n",
        "    agent.critic.load_weights('./save_weights/')\n",
        "\n",
        "    time = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "        action = agent.actor.predict(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        time += 1\n",
        "\n",
        "        print('Time: ', time, 'Reward: ', reward)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}