{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras-0.2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO+odE5Zemw0LKVJizPVO8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinhs0920/19-lab/blob/master/Keras_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHvKYUzRT-wn",
        "colab_type": "text"
      },
      "source": [
        "# 02. 심층 신경망 <br> -  ResNet & DenseNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzfei5cazumu",
        "colab_type": "text"
      },
      "source": [
        "## 목차\n",
        "* **함수형 API**\n",
        "* 두 개의 입력, 한 개의 출력 **Y-Network**\n",
        "* 심층 잔차 신경망(**ResNet**) - v1,v2\n",
        "* 밀집 연결 합성곱 네트워크(**DenseNet**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnvKGgxsT_v1",
        "colab_type": "text"
      },
      "source": [
        "## 함수형 API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqYWvks3Uc7-",
        "colab_type": "text"
      },
      "source": [
        "앞선 순차형 API를 사용한모델로는 구현할 수 없는 더 복잡한 신경망을 구축할 수 있는 도구이다.\n",
        "\n",
        "<BR> 순차형 API와의 차이점은 각 계층은 텐서를 인수로 받으며, 모델은 입력 텐서와 출력텐서 사이의 함수다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAyKrV3MVjzV",
        "colab_type": "text"
      },
      "source": [
        "ex) 함수형 API에서 32개의 필터를 갖는 2차원 합성곱 계층을 Conv2D, 계층 입력텐서를 $x$, 계층 출력텐서를 $y$라 하면 아래와 같이 나타낸다.\n",
        "<br> $y=Conv2D(32)(x)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WjCnQZtUc-9",
        "colab_type": "text"
      },
      "source": [
        "다음 예시는 1장에서의 MNIST코드의 CNN을 함수형 API를 사용하여 나타낸 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6oppwbhSWNT",
        "colab_type": "code",
        "outputId": "4b319f20-ac69-4995-dd8a-bebb22bc4013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# MNIST dataset을 불러옵니다.\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 레이블 개수 계산\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# 원-핫 벡터로 변환\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# 입력 이미지 reshape 및 정규화\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# 신경망 매개변수\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 128\n",
        "kernel_size = 3\n",
        "filters = 64\n",
        "dropout = 0.3\n",
        "\n",
        "# 함수형 API를 사용해 CNN 계층 구축\n",
        "inputs = Input(shape=input_shape)\n",
        "y = Conv2D(filters=filters,  # Convolution : 이미지에 커널, 필터를 입히는 과정 + 특징 맵(커널)의 가장자리가 삭제됨(no-padding에 의해)\n",
        "           kernel_size=kernel_size,\n",
        "           activation='relu')(inputs)\n",
        "y = MaxPooling2D()(y)  # 기본적으로 pool_size=2 (2X2)를 사용하므로 인수를 제거함.\n",
        "y = Conv2D(filters=filters,\n",
        "           kernel_size=kernel_size,\n",
        "           activation='relu')(y)\n",
        "y = MaxPooling2D()(y)\n",
        "y = Conv2D(filters=filters,\n",
        "           kernel_size=kernel_size,\n",
        "           activation='relu')(y)\n",
        "\n",
        "# dense 계층에 연결하기 전 이미지를 알맞은 벡터로 변환\n",
        "y = Flatten()(y)\n",
        "\n",
        "# dropout 정규화\n",
        "y = Dropout(dropout)(y)\n",
        "outputs = Dense(num_labels, activation='softmax')(y)\n",
        "\n",
        "# 입력/출력을 제공해 모델 구축\n",
        "model = Model(inputs=inputs, outputs=outputs) # Model() -> 텐서 리스트를 제공해주는 메서드 \n",
        "\n",
        "# 텍스트로 신경망 모델 요약\n",
        "model.summary()\n",
        "\n",
        "# 분류 모델 손실 함수, Adam 최적화, 정확도\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 입력 이미지와 레이블로 모델 훈련\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          validation_data=(x_test, y_test), # validation_data -> 모델 훈련동안 검증 정확도의 변화 확인을 위한 인수\n",
        "          epochs=20,\n",
        "          batch_size=batch_size)\n",
        "\n",
        "# 테스트 데이터세트에 대한 모델 정확도 평가\n",
        "score = model.evaluate(x_test,\n",
        "                       y_test,\n",
        "                       batch_size=batch_size,\n",
        "                       verbose=0)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5770      \n",
            "=================================================================\n",
            "Total params: 80,266\n",
            "Trainable params: 80,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.2766 - acc: 0.9148 - val_loss: 0.0621 - val_acc: 0.9800\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0723 - acc: 0.9774 - val_loss: 0.0396 - val_acc: 0.9864\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0495 - acc: 0.9845 - val_loss: 0.0304 - val_acc: 0.9901\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0412 - acc: 0.9868 - val_loss: 0.0272 - val_acc: 0.9914\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0343 - acc: 0.9894 - val_loss: 0.0232 - val_acc: 0.9929\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0305 - acc: 0.9898 - val_loss: 0.0241 - val_acc: 0.9922\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0262 - acc: 0.9917 - val_loss: 0.0250 - val_acc: 0.9919\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0209 - val_acc: 0.9931\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0216 - acc: 0.9930 - val_loss: 0.0235 - val_acc: 0.9918\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0199 - acc: 0.9936 - val_loss: 0.0213 - val_acc: 0.9937\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0174 - acc: 0.9942 - val_loss: 0.0267 - val_acc: 0.9912\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0153 - acc: 0.9952 - val_loss: 0.0264 - val_acc: 0.9922\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0152 - acc: 0.9948 - val_loss: 0.0197 - val_acc: 0.9942\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0141 - acc: 0.9953 - val_loss: 0.0212 - val_acc: 0.9928\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0128 - acc: 0.9956 - val_loss: 0.0232 - val_acc: 0.9938\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0120 - acc: 0.9960 - val_loss: 0.0232 - val_acc: 0.9933\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0111 - acc: 0.9960 - val_loss: 0.0269 - val_acc: 0.9921\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0110 - acc: 0.9964 - val_loss: 0.0287 - val_acc: 0.9920\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0086 - acc: 0.9973 - val_loss: 0.0265 - val_acc: 0.9936\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0276 - val_acc: 0.9935\n",
            "\n",
            "Test accuracy: 99.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK1C5sc-Zi2N",
        "colab_type": "text"
      },
      "source": [
        "1장에서 순차적 API를 사용했을 때의 정확도 99.4%와 거의 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO04k6SbZohu",
        "colab_type": "text"
      },
      "source": [
        "##입력이 두 개, 출력이 하나인 모델 생성하기 -> Y-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGE-T2iZokg",
        "colab_type": "text"
      },
      "source": [
        "Y-Network는 동일한 입력을 CNN의 왼쪽과 오른쪽 가지에 두번 사용한다. 이를 concatenate 계층을 사용해 결과를 결합한다.\n",
        "\n",
        "<br>예를 들면 (3,3,16)인 두 개의 텐서를 연결하면 결과로 형상이 (3,3,32)인 텐서를 얻게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU9BX0CBZonf",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73603935-c93bb600-45cc-11ea-93e1-91734c10d600.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjYCF8R5Zorg",
        "colab_type": "text"
      },
      "source": [
        "앞선 코드 모델의 성능을 개선하기 위해 다음 작업을 수행한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IX1jmgiZouG",
        "colab_type": "text"
      },
      "source": [
        "1.Y-Network의 가지에서 필터 수를 두 배로 늘려 MaxPooling2D() 다음에서 특징 맵 크기가 절반으로 줄어든 것을 보완한다. 예를 들어, 첫 번째 합성곱의 출력이 (28,28,32)이면 맥스 풀링 이후에 형상이 (14,14,32)가 된다. 그러면 다음 합성곱 계층의 필터 크기는 64이고 출력은 (14,14,64)이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaJsOfE_ZoxS",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604002-a3fb7780-45cd-11ea-88db-60b991a683f5.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c54sg0F_Zo0I",
        "colab_type": "text"
      },
      "source": [
        "2.오른쪽 한쪽 가지에서 팽창률(dilation rate)을 2로 적용한다. 팽창률을 사용해 커널의 적용 범위를 증가시키면 오른쪽 가지가 더 다양한 특징맵을 학습시킬 수 있다는 뜻이다. padding='same' 을 사용해입력 차원을 출력인 특징 맵과 동일하게 하기 위해 입력의 나머지 부분을 0으로 채워 패딩한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsCMbbqvhDfd",
        "colab_type": "text"
      },
      "source": [
        "다음 코드는 함수형 API를 사용해 Y-Network를 구현하는 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xk-Rmq8hIV2",
        "colab_type": "code",
        "outputId": "783ef113-5e4f-4b11-984f-b6c0c6d3455d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten, concatenate # Y-Network의 특징인 concatenate를 사용하였다.\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 32\n",
        "kernel_size = 3\n",
        "dropout = 0.4\n",
        "n_filters = 32\n",
        "\n",
        "#  Y-Network의 왼쪽 가지\n",
        "left_inputs = Input(shape=input_shape)\n",
        "x = left_inputs\n",
        "filters = n_filters\n",
        "# Conv2D-Dropout-MaxPooling2D 3개의 층으로 구현\n",
        "# 계층이 지날 때 마다 필터 개수를 2배 증가 (32-64-128)\n",
        "for i in range(3):\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               padding='same',\n",
        "               activation='relu')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    filters *= 2\n",
        "\n",
        "# Y-Network의 오른쪽 가지\n",
        "right_inputs = Input(shape=input_shape)\n",
        "y = right_inputs\n",
        "filters = n_filters\n",
        "# Conv2D-Dropout-MaxPooling2D 3개층 구성\n",
        "# 계층이 지날 때 마다 필터 개수를 2배 증가 (32-64-128)\n",
        "for i in range(3):\n",
        "    y = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               padding='same',\n",
        "               activation='relu',\n",
        "               dilation_rate=2)(y)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = MaxPooling2D()(y)\n",
        "    filters *= 2\n",
        "\n",
        "# 왼쪽과 오른쪽 가지 출력 병합\n",
        "y = concatenate([x, y])\n",
        "\n",
        "# Dense 계층에 연결하기 전 특징 맵을 벡터로 변환 \n",
        "y = Flatten()(y)\n",
        "y = Dropout(dropout)(y)\n",
        "outputs = Dense(num_labels, activation='softmax')(y)\n",
        "\n",
        "# 함수형 API에서 모델 구축\n",
        "model = Model([left_inputs, right_inputs], outputs)\n",
        "\n",
        "# 그래프를 사용해 모델 확인\n",
        "plot_model(model, to_file='cnn-y-network.png', show_shapes=True)\n",
        "\n",
        "# 계층 텍스트 설명을 사용해 모델 확인\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit([x_train, x_train],\n",
        "          y_train, \n",
        "          validation_data=([x_test, x_test], y_test),\n",
        "          epochs=20,\n",
        "          batch_size=batch_size)\n",
        "\n",
        "score = model.evaluate([x_test, x_test],\n",
        "                       y_test,\n",
        "                       batch_size=batch_size,\n",
        "                       verbose=0)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 28, 28, 32)   320         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 28, 28, 32)   320         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 28, 28, 32)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 28, 28, 32)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 32)   0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 32)   0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 14, 14, 64)   18496       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 14, 14, 64)   18496       max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 14, 14, 64)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 14, 14, 64)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 7, 7, 64)     0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 7, 7, 64)     0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 7, 7, 128)    73856       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 7, 7, 128)    73856       max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 7, 7, 128)    0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 7, 7, 128)    0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 3, 3, 128)    0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 3, 3, 128)    0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3, 3, 256)    0           max_pooling2d_6[0][0]            \n",
            "                                                                 max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 2304)         0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           23050       dropout_8[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 208,394\n",
            "Trainable params: 208,394\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 14s 241us/sample - loss: 0.1743 - acc: 0.9454 - val_loss: 0.1375 - val_acc: 0.9882\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 14s 229us/sample - loss: 0.0665 - acc: 0.9793 - val_loss: 0.0961 - val_acc: 0.9898\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0535 - acc: 0.9832 - val_loss: 0.1078 - val_acc: 0.9904\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0482 - acc: 0.9851 - val_loss: 0.0673 - val_acc: 0.9911\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0433 - acc: 0.9859 - val_loss: 0.0618 - val_acc: 0.9925\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 14s 231us/sample - loss: 0.0393 - acc: 0.9876 - val_loss: 0.0604 - val_acc: 0.9917\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0388 - acc: 0.9880 - val_loss: 0.0467 - val_acc: 0.9924\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0372 - acc: 0.9882 - val_loss: 0.0706 - val_acc: 0.9935\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0343 - acc: 0.9892 - val_loss: 0.0473 - val_acc: 0.9930\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 14s 230us/sample - loss: 0.0345 - acc: 0.9892 - val_loss: 0.0391 - val_acc: 0.9942\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0329 - acc: 0.9898 - val_loss: 0.0584 - val_acc: 0.9931\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 14s 227us/sample - loss: 0.0345 - acc: 0.9893 - val_loss: 0.0385 - val_acc: 0.9939\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 13s 217us/sample - loss: 0.0304 - acc: 0.9902 - val_loss: 0.0354 - val_acc: 0.9941\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 13s 218us/sample - loss: 0.0316 - acc: 0.9900 - val_loss: 0.0308 - val_acc: 0.9931\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 13s 220us/sample - loss: 0.0316 - acc: 0.9903 - val_loss: 0.0620 - val_acc: 0.9934\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 13s 220us/sample - loss: 0.0297 - acc: 0.9909 - val_loss: 0.0332 - val_acc: 0.9938\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 13s 220us/sample - loss: 0.0316 - acc: 0.9905 - val_loss: 0.0315 - val_acc: 0.9949\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 13s 219us/sample - loss: 0.0302 - acc: 0.9905 - val_loss: 0.0443 - val_acc: 0.9931\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 13s 218us/sample - loss: 0.0318 - acc: 0.9905 - val_loss: 0.0392 - val_acc: 0.9918\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 13s 217us/sample - loss: 0.0299 - acc: 0.9909 - val_loss: 0.0378 - val_acc: 0.9909\n",
            "\n",
            "Test accuracy: 99.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axpfHqEMiqNk",
        "colab_type": "text"
      },
      "source": [
        "또 하나의 특징은 Y-Network에서는 훈련과 검증에 두 개의 입력이 필요하므로, [x-train, x-train]이 제공된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baQIm-S5jo1m",
        "colab_type": "text"
      },
      "source": [
        "아래 그림은  위 코드를 구현한 CNN MNIST Y-Network이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHFcus71jUem",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604144-5849cd80-45cf-11ea-8dba-f183d6041867.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4hY67fDj1pd",
        "colab_type": "text"
      },
      "source": [
        "# 심층 잔차 신경망(ResNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgaTzXPDj10n",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604184-cee6cb00-45cf-11ea-9869-acd291c41730.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ8m33CMj1sk",
        "colab_type": "text"
      },
      "source": [
        "심층 신경망에서 문제는 역전파가 이루어지면서 그레디언트 손실이 발생하는 것이다.\n",
        "<br>역전파는 연쇄법칙을 기반으로 하며 그레디언트는 입력층으로 갈수록 손실이 진행된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxOoenCkj1xt",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604231-677d4b00-45d0-11ea-8ef7-21f17cd90f94.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TfuwDXBj13k",
        "colab_type": "text"
      },
      "source": [
        "  ResNet 블록은 일반 CNN 블록과 비교했을 때 역전파 중 그레디언트 손실을 막기위해 **숏컷 연결**을 도입하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuzJ2ofTl_21",
        "colab_type": "text"
      },
      "source": [
        "### CNN과 ResNet 차이점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQGDxfAJj16W",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604261-eecabe80-45d0-11ea-9d06-8a3f022b1280.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgsAMy2_l_7y",
        "colab_type": "text"
      },
      "source": [
        "##### CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFBWiYrMl__D",
        "colab_type": "text"
      },
      "source": [
        "CNN 계층의 연산은 Conv2D - 배치 정규화- ReLU 를 통하여 진행된다. \n",
        "\n",
        "<br>즉 특정 계층 l의 출력 특징 맵은 이전 특징 맵에서만 직접적으로 영향받는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY7K7_dfmACY",
        "colab_type": "text"
      },
      "source": [
        "##### ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R04HCcqmAFf",
        "colab_type": "text"
      },
      "source": [
        "ResNet 계층 연산은  Conv2D - 배치 정규화- ReLU 연산을 진행한 출력이  Conv2D - 배치 정규화 과정(잔차 매핑)의 연산을 한번 더 진행한 출력과 원래 입력값을 더하여 ReLU 함수 연산을 진행하여 출력을 낸다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMucUqRkmAIm",
        "colab_type": "text"
      },
      "source": [
        "이에 대한 예제로는 CIFAR10을 활용할 것이다.\n",
        "<br> CIFAR10 역시 MNIST처럼 10개의 카테고리로 되어 있다. (32X32)의 RGB로 되어있다. 5만개의 레이블이 달린 훈련 이미지와, 1만개의 검증 이미지로 구성되어 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9aLkE_jqPrc",
        "colab_type": "text"
      },
      "source": [
        "아래 그림은 CIFAR10 데이터세트에 대한 ResNet 모델 아키텍처를 보여준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S50OUOFHpRhN",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73604362-eecbbe00-45d2-11ea-9d0b-cd615276b38a.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0gJ3WXYpRk8",
        "colab_type": "text"
      },
      "source": [
        "아래 표는 ResNet 아키텍쳐 구성이다.\n",
        "![image](https://user-images.githubusercontent.com/53015968/73604469-b5944d80-45d4-11ea-8301-6d74a02f3b30.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFamkg1opRpS",
        "colab_type": "text"
      },
      "source": [
        "이는 잔차 블록 집합이 3개 있다는 뜻이며, 각 집합에는 n개의 잔차 블록이 있다면 2n개의 계층이 있다.\n",
        "\n",
        "<br> 서로 다른 크기의 두 특징 맵 사이의 전이를 제외하면 커널 크기는 전부 3 이다.전이 계층에서는 커널 크기가 1이고 strides=2 일때의 Conv2D이다.\n",
        "\n",
        "*) strides=2 뜻은 합성곱 적용 시, 픽셀을 하나씩 건너뛰면서 커널을 적용\n",
        "\n",
        "<BR> 서로 다른 크기의 두 잔차 블록을 연결할 때 전이 계층을 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKC97Z2UpRtJ",
        "colab_type": "text"
      },
      "source": [
        "마지막 계층은 AveragePooling2D-Faltten-Dense로 구성된다. ResNet은 dropout을 사용하지 않는다. 또한, 덧셈 연산과 1X1 합성곱 적용은 자신을 정규화하는 효과를 가진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwV5B_k6pRwe",
        "colab_type": "text"
      },
      "source": [
        "CIFAR10 데이터세트 분류를 위한 ResNet 아키텍쳐\n",
        "![image](https://user-images.githubusercontent.com/53015968/73604527-b1b4fb00-45d5-11ea-99c5-b54ee8b50375.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWuLuAVAuKzU",
        "colab_type": "code",
        "outputId": "59035d1a-bb93-44d3-b108-d393196e65b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# ResNet v1과 ResNet v2가 모두 구현되어 있다.\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input\n",
        "from tensorflow.keras.layers import Flatten, add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 훈련 매개변수\n",
        "batch_size = 32\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# 모델 파라미터\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# 모델 버전\n",
        "# 최초 논문: version = 1 (ResNet v1), \n",
        "version = 1\n",
        "\n",
        "# 제공된 모델 매개변수 n으로부터 계산된 네트워크 깊이\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# CIFAR10 데이터 불러오기\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# 정규화\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# if subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\" 학습률은 lr_schedule()을 통해 80, 120, 160, 180 epochs 에서 시작하여 점차 감소시킨다.\n",
        "    기본값은 1e-3 이다. lr_schedule()함수는 모델 훈련동안 한 epoch 당 callbacks 변수로 호출된다.\n",
        "\n",
        "    # Arguments\n",
        "        에폭의 수 :-> epoch (int)\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "    Arguments:\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "    Returns:\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal', # 역전파 시 수렴될 수 있게 해주는 것\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, in [a])')\n",
        "    \n",
        "    # 모델 정의 시작\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    \n",
        "    # 잔차 유닛 인스턴스화\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "           \n",
        "            if stack > 0 and res_block == 0:  \n",
        "                strides = 2  \n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            \n",
        "            if stack > 0 and res_block == 0:\n",
        "               \n",
        "               # 변경된 차원을 맞추기 위해 잔차 숏컷 연결을 선형으로 사영 \n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # 분류기 추가\n",
        "    # v1은 마지막 숏컷 연결-ReLU 후에는 BN을 사용하지 않는다.\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # 모델 인스턴스화\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "#ResNet v2 구현\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "   \n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 110 in [b])')\n",
        "   \n",
        "   # 모델 정의\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "   # v2에서는 2 경로로 나뉘기 전에 입력에 BN-ReLU와 함께 Conv2D를 수행 \n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    \n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                # first layer and first stage\n",
        "                if res_block == 0:  \n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "               \n",
        "                if res_block == 0:\n",
        "                   \n",
        "                    strides = 2 \n",
        "\n",
        "           \n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "              \n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "plot_model(model, to_file=\"%s.png\" % model_type, show_shapes=True)\n",
        "print(model_type)\n",
        "\n",
        "# prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # this will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False)\n",
        "\n",
        "    # compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, \n",
        "                        steps_per_epoch=len(x_train)//batch_size,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# score trained model\n",
        "scores = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Learning rate:  0.001\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 32)   4640        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 32)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 32)   9248        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 32)   544         activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 32)   0           conv2d_9[0][0]                   \n",
            "                                                                 batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 32)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 32)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 32)   0           activation_8[0][0]               \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 32)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 32)   0           activation_10[0][0]              \n",
            "                                                                 batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 64)     18496       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 64)     36928       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 64)     2112        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 64)     0           conv2d_16[0][0]                  \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 64)     0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 64)     0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 64)     0           activation_14[0][0]              \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 64)     0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 64)     0           activation_16[0][0]              \n",
            "                                                                 batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n",
            "Using real-time data augmentation.\n",
            "Learning rate:  0.001\n",
            "Epoch 1/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 1.5828 - acc: 0.4857Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 3s 261us/sample - loss: 1.7077 - acc: 0.4979\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 51s 33ms/step - loss: 1.5822 - acc: 0.4861 - val_loss: 1.6332 - val_acc: 0.4979\n",
            "Learning rate:  0.001\n",
            "Epoch 2/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 1.1831 - acc: 0.6359Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 1.2696 - acc: 0.5962\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.1830 - acc: 0.6359 - val_loss: 1.3643 - val_acc: 0.5962\n",
            "Learning rate:  0.001\n",
            "Epoch 3/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 1.0126 - acc: 0.7031Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 214us/sample - loss: 1.3817 - acc: 0.5804\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 1.0126 - acc: 0.7030 - val_loss: 1.5564 - val_acc: 0.5804\n",
            "Learning rate:  0.001\n",
            "Epoch 4/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.9220 - acc: 0.7358Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 210us/sample - loss: 0.9769 - acc: 0.7019\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.9219 - acc: 0.7359 - val_loss: 1.0454 - val_acc: 0.7019\n",
            "Learning rate:  0.001\n",
            "Epoch 5/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.8615 - acc: 0.7594Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 203us/sample - loss: 1.2228 - acc: 0.6796\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.8614 - acc: 0.7595 - val_loss: 1.1469 - val_acc: 0.6796\n",
            "Learning rate:  0.001\n",
            "Epoch 6/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.8176 - acc: 0.7762Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 206us/sample - loss: 0.8101 - acc: 0.7640\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.8178 - acc: 0.7761 - val_loss: 0.8441 - val_acc: 0.7640\n",
            "Learning rate:  0.001\n",
            "Epoch 7/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.7832 - acc: 0.7887Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 210us/sample - loss: 0.7620 - acc: 0.7677\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7833 - acc: 0.7886 - val_loss: 0.8838 - val_acc: 0.7677\n",
            "Learning rate:  0.001\n",
            "Epoch 8/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7578 - acc: 0.8006Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 207us/sample - loss: 0.7702 - acc: 0.7561\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.7577 - acc: 0.8006 - val_loss: 0.8797 - val_acc: 0.7561\n",
            "Learning rate:  0.001\n",
            "Epoch 9/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.7289 - acc: 0.8111Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 211us/sample - loss: 0.7237 - acc: 0.7992\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.7288 - acc: 0.8111 - val_loss: 0.7678 - val_acc: 0.7992\n",
            "Learning rate:  0.001\n",
            "Epoch 10/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.7142 - acc: 0.8150Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 207us/sample - loss: 0.7628 - acc: 0.7710\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.7143 - acc: 0.8149 - val_loss: 0.8836 - val_acc: 0.7710\n",
            "Learning rate:  0.001\n",
            "Epoch 11/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.8236Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 208us/sample - loss: 0.7789 - acc: 0.7777\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6915 - acc: 0.8236 - val_loss: 0.8608 - val_acc: 0.7777\n",
            "Learning rate:  0.001\n",
            "Epoch 12/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.6812 - acc: 0.8292Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 209us/sample - loss: 0.8230 - acc: 0.7783\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6811 - acc: 0.8293 - val_loss: 0.8723 - val_acc: 0.7783\n",
            "Learning rate:  0.001\n",
            "Epoch 13/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.6639 - acc: 0.8366Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 204us/sample - loss: 0.8038 - acc: 0.7863\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 25ms/step - loss: 0.6639 - acc: 0.8366 - val_loss: 0.8474 - val_acc: 0.7863\n",
            "Learning rate:  0.001\n",
            "Epoch 14/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.8379Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 203us/sample - loss: 0.6723 - acc: 0.8017\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 25ms/step - loss: 0.6573 - acc: 0.8378 - val_loss: 0.7804 - val_acc: 0.8017\n",
            "Learning rate:  0.001\n",
            "Epoch 15/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.6493 - acc: 0.8422Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 214us/sample - loss: 0.7409 - acc: 0.7899\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6493 - acc: 0.8422 - val_loss: 0.8259 - val_acc: 0.7899\n",
            "Learning rate:  0.001\n",
            "Epoch 16/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.6381 - acc: 0.8463Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 206us/sample - loss: 0.6079 - acc: 0.8085\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.6380 - acc: 0.8463 - val_loss: 0.7756 - val_acc: 0.8085\n",
            "Learning rate:  0.001\n",
            "Epoch 17/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.6299 - acc: 0.8471Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.7011 - acc: 0.8082\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.6299 - acc: 0.8471 - val_loss: 0.7708 - val_acc: 0.8082\n",
            "Learning rate:  0.001\n",
            "Epoch 18/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.6176 - acc: 0.8550Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 207us/sample - loss: 0.8925 - acc: 0.8013\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 25ms/step - loss: 0.6177 - acc: 0.8550 - val_loss: 0.7956 - val_acc: 0.8013\n",
            "Learning rate:  0.001\n",
            "Epoch 19/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.6185 - acc: 0.8525Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 218us/sample - loss: 0.7078 - acc: 0.8050\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.6185 - acc: 0.8525 - val_loss: 0.7910 - val_acc: 0.8050\n",
            "Learning rate:  0.001\n",
            "Epoch 20/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.6085 - acc: 0.8589Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.8038 - acc: 0.8405\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.6083 - acc: 0.8590 - val_loss: 0.6641 - val_acc: 0.8405\n",
            "Learning rate:  0.001\n",
            "Epoch 21/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.8597Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 219us/sample - loss: 0.6481 - acc: 0.8300\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.6030 - acc: 0.8597 - val_loss: 0.6992 - val_acc: 0.8300\n",
            "Learning rate:  0.001\n",
            "Epoch 22/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5957 - acc: 0.8626Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.8427 - acc: 0.7744\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5958 - acc: 0.8625 - val_loss: 0.9481 - val_acc: 0.7744\n",
            "Learning rate:  0.001\n",
            "Epoch 23/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5923 - acc: 0.8633Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.6383 - acc: 0.8232\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.5925 - acc: 0.8632 - val_loss: 0.7225 - val_acc: 0.8232\n",
            "Learning rate:  0.001\n",
            "Epoch 24/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5856 - acc: 0.8664Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 205us/sample - loss: 0.6460 - acc: 0.8259\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5857 - acc: 0.8663 - val_loss: 0.7142 - val_acc: 0.8259\n",
            "Learning rate:  0.001\n",
            "Epoch 25/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.8681Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 219us/sample - loss: 0.6279 - acc: 0.8407\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5787 - acc: 0.8681 - val_loss: 0.6747 - val_acc: 0.8407\n",
            "Learning rate:  0.001\n",
            "Epoch 26/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5823 - acc: 0.8681Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.7453 - acc: 0.8288\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5823 - acc: 0.8681 - val_loss: 0.7268 - val_acc: 0.8288\n",
            "Learning rate:  0.001\n",
            "Epoch 27/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5728 - acc: 0.8710Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 209us/sample - loss: 0.8290 - acc: 0.7912\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5730 - acc: 0.8709 - val_loss: 0.8893 - val_acc: 0.7912\n",
            "Learning rate:  0.001\n",
            "Epoch 28/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5730 - acc: 0.8726Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.8281 - acc: 0.8004\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5729 - acc: 0.8727 - val_loss: 0.8048 - val_acc: 0.8004\n",
            "Learning rate:  0.001\n",
            "Epoch 29/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.8722Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.7252 - acc: 0.8339\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5694 - acc: 0.8722 - val_loss: 0.7127 - val_acc: 0.8339\n",
            "Learning rate:  0.001\n",
            "Epoch 30/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.8719Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.6429 - acc: 0.8372\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5697 - acc: 0.8718 - val_loss: 0.6853 - val_acc: 0.8372\n",
            "Learning rate:  0.001\n",
            "Epoch 31/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5605 - acc: 0.8778Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 204us/sample - loss: 0.8717 - acc: 0.7937\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5606 - acc: 0.8778 - val_loss: 0.9398 - val_acc: 0.7937\n",
            "Learning rate:  0.001\n",
            "Epoch 32/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.8771Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.7563 - acc: 0.8250\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5577 - acc: 0.8772 - val_loss: 0.7223 - val_acc: 0.8250\n",
            "Learning rate:  0.001\n",
            "Epoch 33/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.8788Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 193us/sample - loss: 0.6974 - acc: 0.8462\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5553 - acc: 0.8787 - val_loss: 0.6650 - val_acc: 0.8462\n",
            "Learning rate:  0.001\n",
            "Epoch 34/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.8805Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.6714 - acc: 0.8417\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5499 - acc: 0.8805 - val_loss: 0.7100 - val_acc: 0.8417\n",
            "Learning rate:  0.001\n",
            "Epoch 35/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5528 - acc: 0.8799Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 194us/sample - loss: 0.5686 - acc: 0.8158\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5528 - acc: 0.8799 - val_loss: 0.7578 - val_acc: 0.8158\n",
            "Learning rate:  0.001\n",
            "Epoch 36/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.8806Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 206us/sample - loss: 0.7180 - acc: 0.8417\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5493 - acc: 0.8807 - val_loss: 0.6739 - val_acc: 0.8417\n",
            "Learning rate:  0.001\n",
            "Epoch 37/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8811Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.9253 - acc: 0.8266\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5474 - acc: 0.8810 - val_loss: 0.7441 - val_acc: 0.8266\n",
            "Learning rate:  0.001\n",
            "Epoch 38/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.8829Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 204us/sample - loss: 0.8650 - acc: 0.7995\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5432 - acc: 0.8829 - val_loss: 0.8547 - val_acc: 0.7995\n",
            "Learning rate:  0.001\n",
            "Epoch 39/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.8817Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 206us/sample - loss: 0.7124 - acc: 0.8460\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5453 - acc: 0.8817 - val_loss: 0.6650 - val_acc: 0.8460\n",
            "Learning rate:  0.001\n",
            "Epoch 40/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8856Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 205us/sample - loss: 0.8119 - acc: 0.8351\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5374 - acc: 0.8855 - val_loss: 0.7244 - val_acc: 0.8351\n",
            "Learning rate:  0.001\n",
            "Epoch 41/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8850Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.6917 - acc: 0.8458\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5383 - acc: 0.8849 - val_loss: 0.6775 - val_acc: 0.8458\n",
            "Learning rate:  0.001\n",
            "Epoch 42/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5391 - acc: 0.8848Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.8342 - acc: 0.7957\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5390 - acc: 0.8848 - val_loss: 0.8742 - val_acc: 0.7957\n",
            "Learning rate:  0.001\n",
            "Epoch 43/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.8854Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.7903 - acc: 0.8023\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5392 - acc: 0.8854 - val_loss: 0.8273 - val_acc: 0.8023\n",
            "Learning rate:  0.001\n",
            "Epoch 44/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.8867Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 208us/sample - loss: 0.6812 - acc: 0.8440\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5327 - acc: 0.8867 - val_loss: 0.7138 - val_acc: 0.8440\n",
            "Learning rate:  0.001\n",
            "Epoch 45/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5342 - acc: 0.8850Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.7096 - acc: 0.8157\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5344 - acc: 0.8850 - val_loss: 0.7797 - val_acc: 0.8157\n",
            "Learning rate:  0.001\n",
            "Epoch 46/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8860Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.7175 - acc: 0.8025\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5347 - acc: 0.8860 - val_loss: 0.8549 - val_acc: 0.8025\n",
            "Learning rate:  0.001\n",
            "Epoch 47/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.8900Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 203us/sample - loss: 0.6433 - acc: 0.8496\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5270 - acc: 0.8900 - val_loss: 0.6699 - val_acc: 0.8496\n",
            "Learning rate:  0.001\n",
            "Epoch 48/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.8894Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 207us/sample - loss: 0.6696 - acc: 0.8603\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5286 - acc: 0.8894 - val_loss: 0.6259 - val_acc: 0.8603\n",
            "Learning rate:  0.001\n",
            "Epoch 49/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.8873Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 206us/sample - loss: 0.9005 - acc: 0.8159\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5270 - acc: 0.8873 - val_loss: 0.8251 - val_acc: 0.8159\n",
            "Learning rate:  0.001\n",
            "Epoch 50/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.8888Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 205us/sample - loss: 0.8202 - acc: 0.8375\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5252 - acc: 0.8887 - val_loss: 0.7307 - val_acc: 0.8375\n",
            "Learning rate:  0.001\n",
            "Epoch 51/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.8904Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.7571 - acc: 0.8354\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5241 - acc: 0.8904 - val_loss: 0.7206 - val_acc: 0.8354\n",
            "Learning rate:  0.001\n",
            "Epoch 52/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.8905Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 205us/sample - loss: 0.5816 - acc: 0.8519\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5236 - acc: 0.8905 - val_loss: 0.6682 - val_acc: 0.8519\n",
            "Learning rate:  0.001\n",
            "Epoch 53/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.8907Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.6255 - acc: 0.8573\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5208 - acc: 0.8907 - val_loss: 0.6469 - val_acc: 0.8573\n",
            "Learning rate:  0.001\n",
            "Epoch 54/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.8886Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.7538 - acc: 0.8513\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5236 - acc: 0.8886 - val_loss: 0.6668 - val_acc: 0.8513\n",
            "Learning rate:  0.001\n",
            "Epoch 55/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8915Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.6624 - acc: 0.8560\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 40s 25ms/step - loss: 0.5197 - acc: 0.8915 - val_loss: 0.6364 - val_acc: 0.8560\n",
            "Learning rate:  0.001\n",
            "Epoch 56/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8912Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.6832 - acc: 0.8714\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5197 - acc: 0.8912 - val_loss: 0.6011 - val_acc: 0.8714\n",
            "Learning rate:  0.001\n",
            "Epoch 57/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.8946Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 191us/sample - loss: 0.8820 - acc: 0.8365\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5122 - acc: 0.8947 - val_loss: 0.7016 - val_acc: 0.8365\n",
            "Learning rate:  0.001\n",
            "Epoch 58/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.8918Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.6976 - acc: 0.8151\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5196 - acc: 0.8918 - val_loss: 0.7779 - val_acc: 0.8151\n",
            "Learning rate:  0.001\n",
            "Epoch 59/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.8930Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.8418 - acc: 0.8453\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5149 - acc: 0.8930 - val_loss: 0.6942 - val_acc: 0.8453\n",
            "Learning rate:  0.001\n",
            "Epoch 60/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.8953Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 209us/sample - loss: 0.9050 - acc: 0.8360\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5111 - acc: 0.8953 - val_loss: 0.7240 - val_acc: 0.8360\n",
            "Learning rate:  0.001\n",
            "Epoch 61/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.8948Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.9113 - acc: 0.7799\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5142 - acc: 0.8948 - val_loss: 0.9823 - val_acc: 0.7799\n",
            "Learning rate:  0.001\n",
            "Epoch 62/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5126 - acc: 0.8940Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.6725 - acc: 0.8554\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5126 - acc: 0.8940 - val_loss: 0.6571 - val_acc: 0.8554\n",
            "Learning rate:  0.001\n",
            "Epoch 63/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.8937Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.9596 - acc: 0.7974\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5154 - acc: 0.8936 - val_loss: 0.9296 - val_acc: 0.7974\n",
            "Learning rate:  0.001\n",
            "Epoch 64/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8955Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 204us/sample - loss: 0.6597 - acc: 0.8653\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5065 - acc: 0.8955 - val_loss: 0.6304 - val_acc: 0.8653\n",
            "Learning rate:  0.001\n",
            "Epoch 65/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5071 - acc: 0.8955Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 210us/sample - loss: 0.6764 - acc: 0.8352\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5068 - acc: 0.8956 - val_loss: 0.7302 - val_acc: 0.8352\n",
            "Learning rate:  0.001\n",
            "Epoch 66/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5042 - acc: 0.8975Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.6142 - acc: 0.8376\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5041 - acc: 0.8975 - val_loss: 0.7098 - val_acc: 0.8376\n",
            "Learning rate:  0.001\n",
            "Epoch 67/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.8945Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 203us/sample - loss: 0.6901 - acc: 0.8403\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5115 - acc: 0.8946 - val_loss: 0.7084 - val_acc: 0.8403\n",
            "Learning rate:  0.001\n",
            "Epoch 68/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.8950Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 213us/sample - loss: 0.7445 - acc: 0.8597\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5088 - acc: 0.8950 - val_loss: 0.6486 - val_acc: 0.8597\n",
            "Learning rate:  0.001\n",
            "Epoch 69/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8978Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.7906 - acc: 0.8125\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5037 - acc: 0.8979 - val_loss: 0.8366 - val_acc: 0.8125\n",
            "Learning rate:  0.001\n",
            "Epoch 70/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.8978Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 208us/sample - loss: 0.9104 - acc: 0.7951\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5041 - acc: 0.8977 - val_loss: 0.8948 - val_acc: 0.7951\n",
            "Learning rate:  0.001\n",
            "Epoch 71/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5030 - acc: 0.8960Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.7750 - acc: 0.8543\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5030 - acc: 0.8959 - val_loss: 0.6457 - val_acc: 0.8543\n",
            "Learning rate:  0.001\n",
            "Epoch 72/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5041 - acc: 0.8956Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 203us/sample - loss: 0.7317 - acc: 0.8321\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5041 - acc: 0.8956 - val_loss: 0.7255 - val_acc: 0.8321\n",
            "Learning rate:  0.001\n",
            "Epoch 73/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.4982 - acc: 0.8978Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 193us/sample - loss: 1.0329 - acc: 0.7816\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.4981 - acc: 0.8978 - val_loss: 0.9398 - val_acc: 0.7816\n",
            "Learning rate:  0.001\n",
            "Epoch 74/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.8962Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 192us/sample - loss: 0.6118 - acc: 0.8607\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.5038 - acc: 0.8964 - val_loss: 0.6425 - val_acc: 0.8607\n",
            "Learning rate:  0.001\n",
            "Epoch 75/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.5004 - acc: 0.8973Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 192us/sample - loss: 1.0366 - acc: 0.7895\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5002 - acc: 0.8974 - val_loss: 0.9297 - val_acc: 0.7895\n",
            "Learning rate:  0.001\n",
            "Epoch 76/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.5005 - acc: 0.8977Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 193us/sample - loss: 0.6289 - acc: 0.8440\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.5006 - acc: 0.8977 - val_loss: 0.7017 - val_acc: 0.8440\n",
            "Learning rate:  0.001\n",
            "Epoch 77/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.8980Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 203us/sample - loss: 0.6620 - acc: 0.8651\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.5005 - acc: 0.8981 - val_loss: 0.6210 - val_acc: 0.8651\n",
            "Learning rate:  0.001\n",
            "Epoch 78/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.8997Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.6058 - acc: 0.8677\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.4964 - acc: 0.8998 - val_loss: 0.6178 - val_acc: 0.8677\n",
            "Learning rate:  0.001\n",
            "Epoch 79/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.8992Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.6148 - acc: 0.8589\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.4966 - acc: 0.8992 - val_loss: 0.6319 - val_acc: 0.8589\n",
            "Learning rate:  0.001\n",
            "Epoch 80/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.9015Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.6121 - acc: 0.8539\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.4945 - acc: 0.9015 - val_loss: 0.6689 - val_acc: 0.8539\n",
            "Learning rate:  0.001\n",
            "Epoch 81/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.4960 - acc: 0.9006Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 211us/sample - loss: 0.6050 - acc: 0.8426\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.4960 - acc: 0.9006 - val_loss: 0.7160 - val_acc: 0.8426\n",
            "Learning rate:  0.0001\n",
            "Epoch 82/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.9270Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 192us/sample - loss: 0.4203 - acc: 0.8993\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.4155 - acc: 0.9270 - val_loss: 0.5034 - val_acc: 0.8993\n",
            "Learning rate:  0.0001\n",
            "Epoch 83/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.9373Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 207us/sample - loss: 0.4387 - acc: 0.9058\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.3830 - acc: 0.9373 - val_loss: 0.4858 - val_acc: 0.9058\n",
            "Learning rate:  0.0001\n",
            "Epoch 84/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.9413Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4147 - acc: 0.9071\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.3679 - acc: 0.9414 - val_loss: 0.4860 - val_acc: 0.9071\n",
            "Learning rate:  0.0001\n",
            "Epoch 85/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.9438Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4299 - acc: 0.9109\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.3555 - acc: 0.9438 - val_loss: 0.4739 - val_acc: 0.9109\n",
            "Learning rate:  0.0001\n",
            "Epoch 86/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.9467Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.4157 - acc: 0.9105\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.3431 - acc: 0.9467 - val_loss: 0.4725 - val_acc: 0.9105\n",
            "Learning rate:  0.0001\n",
            "Epoch 87/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.3375 - acc: 0.9469Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4097 - acc: 0.9112\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.3375 - acc: 0.9469 - val_loss: 0.4730 - val_acc: 0.9112\n",
            "Learning rate:  0.0001\n",
            "Epoch 88/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.9508Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.4258 - acc: 0.9114\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.3258 - acc: 0.9508 - val_loss: 0.4670 - val_acc: 0.9114\n",
            "Learning rate:  0.0001\n",
            "Epoch 89/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.9512Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4426 - acc: 0.9088\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.3230 - acc: 0.9512 - val_loss: 0.4782 - val_acc: 0.9088\n",
            "Learning rate:  0.0001\n",
            "Epoch 90/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9530Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.4425 - acc: 0.9116\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.3142 - acc: 0.9529 - val_loss: 0.4600 - val_acc: 0.9116\n",
            "Learning rate:  0.0001\n",
            "Epoch 91/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.3112 - acc: 0.9529Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4521 - acc: 0.9131\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.3112 - acc: 0.9528 - val_loss: 0.4530 - val_acc: 0.9131\n",
            "Learning rate:  0.0001\n",
            "Epoch 92/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9560Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4254 - acc: 0.9101\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.3024 - acc: 0.9559 - val_loss: 0.4550 - val_acc: 0.9101\n",
            "Learning rate:  0.0001\n",
            "Epoch 93/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9558Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 194us/sample - loss: 0.4417 - acc: 0.9119\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2998 - acc: 0.9559 - val_loss: 0.4503 - val_acc: 0.9119\n",
            "Learning rate:  0.0001\n",
            "Epoch 94/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9568Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4484 - acc: 0.9094\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2936 - acc: 0.9568 - val_loss: 0.4572 - val_acc: 0.9094\n",
            "Learning rate:  0.0001\n",
            "Epoch 95/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9572Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 203us/sample - loss: 0.4508 - acc: 0.9110\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2902 - acc: 0.9572 - val_loss: 0.4547 - val_acc: 0.9110\n",
            "Learning rate:  0.0001\n",
            "Epoch 96/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9599Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4694 - acc: 0.9086\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2807 - acc: 0.9600 - val_loss: 0.4653 - val_acc: 0.9086\n",
            "Learning rate:  0.0001\n",
            "Epoch 97/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9591Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.4733 - acc: 0.9160\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2812 - acc: 0.9591 - val_loss: 0.4400 - val_acc: 0.9160\n",
            "Learning rate:  0.0001\n",
            "Epoch 98/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9603Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4417 - acc: 0.9097\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2768 - acc: 0.9603 - val_loss: 0.4552 - val_acc: 0.9097\n",
            "Learning rate:  0.0001\n",
            "Epoch 99/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9600Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 207us/sample - loss: 0.4253 - acc: 0.9110\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2727 - acc: 0.9600 - val_loss: 0.4471 - val_acc: 0.9110\n",
            "Learning rate:  0.0001\n",
            "Epoch 100/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.2678 - acc: 0.9624Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 206us/sample - loss: 0.4870 - acc: 0.9151\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2678 - acc: 0.9623 - val_loss: 0.4383 - val_acc: 0.9151\n",
            "Learning rate:  0.0001\n",
            "Epoch 101/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2648 - acc: 0.9631Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.5013 - acc: 0.9140\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2648 - acc: 0.9631 - val_loss: 0.4397 - val_acc: 0.9140\n",
            "Learning rate:  0.0001\n",
            "Epoch 102/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9633Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4801 - acc: 0.9116\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2605 - acc: 0.9633 - val_loss: 0.4442 - val_acc: 0.9116\n",
            "Learning rate:  0.0001\n",
            "Epoch 103/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9635Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.4416 - acc: 0.9118\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2595 - acc: 0.9635 - val_loss: 0.4529 - val_acc: 0.9118\n",
            "Learning rate:  0.0001\n",
            "Epoch 104/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9628Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 190us/sample - loss: 0.4993 - acc: 0.9138\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2576 - acc: 0.9628 - val_loss: 0.4392 - val_acc: 0.9138\n",
            "Learning rate:  0.0001\n",
            "Epoch 105/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.9651Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 205us/sample - loss: 0.4229 - acc: 0.9097\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2510 - acc: 0.9651 - val_loss: 0.4542 - val_acc: 0.9097\n",
            "Learning rate:  0.0001\n",
            "Epoch 106/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9648Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 194us/sample - loss: 0.3870 - acc: 0.9117\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2467 - acc: 0.9648 - val_loss: 0.4519 - val_acc: 0.9117\n",
            "Learning rate:  0.0001\n",
            "Epoch 107/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9666Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4573 - acc: 0.9114\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2438 - acc: 0.9667 - val_loss: 0.4412 - val_acc: 0.9114\n",
            "Learning rate:  0.0001\n",
            "Epoch 108/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9651Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 194us/sample - loss: 0.4517 - acc: 0.9130\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2427 - acc: 0.9651 - val_loss: 0.4368 - val_acc: 0.9130\n",
            "Learning rate:  0.0001\n",
            "Epoch 109/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9656Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 205us/sample - loss: 0.4521 - acc: 0.9146\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2416 - acc: 0.9656 - val_loss: 0.4340 - val_acc: 0.9146\n",
            "Learning rate:  0.0001\n",
            "Epoch 110/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9667Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 209us/sample - loss: 0.5110 - acc: 0.9119\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2391 - acc: 0.9667 - val_loss: 0.4381 - val_acc: 0.9119\n",
            "Learning rate:  0.0001\n",
            "Epoch 111/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9649Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4660 - acc: 0.9116\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2393 - acc: 0.9649 - val_loss: 0.4350 - val_acc: 0.9116\n",
            "Learning rate:  0.0001\n",
            "Epoch 112/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9672Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 214us/sample - loss: 0.4750 - acc: 0.9152\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2330 - acc: 0.9672 - val_loss: 0.4332 - val_acc: 0.9152\n",
            "Learning rate:  0.0001\n",
            "Epoch 113/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9688Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 211us/sample - loss: 0.4834 - acc: 0.9134\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2273 - acc: 0.9688 - val_loss: 0.4343 - val_acc: 0.9134\n",
            "Learning rate:  0.0001\n",
            "Epoch 114/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9687Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.5060 - acc: 0.9112\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2275 - acc: 0.9687 - val_loss: 0.4478 - val_acc: 0.9112\n",
            "Learning rate:  0.0001\n",
            "Epoch 115/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9682Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 190us/sample - loss: 0.4146 - acc: 0.9052\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2266 - acc: 0.9682 - val_loss: 0.4648 - val_acc: 0.9052\n",
            "Learning rate:  0.0001\n",
            "Epoch 116/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9693Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4021 - acc: 0.9112\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2220 - acc: 0.9693 - val_loss: 0.4396 - val_acc: 0.9112\n",
            "Learning rate:  0.0001\n",
            "Epoch 117/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9698Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.5241 - acc: 0.9127\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2200 - acc: 0.9698 - val_loss: 0.4391 - val_acc: 0.9127\n",
            "Learning rate:  0.0001\n",
            "Epoch 118/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9692Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.5271 - acc: 0.9107\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2211 - acc: 0.9692 - val_loss: 0.4466 - val_acc: 0.9107\n",
            "Learning rate:  0.0001\n",
            "Epoch 119/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9710Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.5109 - acc: 0.9126\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2162 - acc: 0.9710 - val_loss: 0.4352 - val_acc: 0.9126\n",
            "Learning rate:  0.0001\n",
            "Epoch 120/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9702Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.5255 - acc: 0.9122\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.2148 - acc: 0.9702 - val_loss: 0.4311 - val_acc: 0.9122\n",
            "Learning rate:  0.0001\n",
            "Epoch 121/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9717Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.4749 - acc: 0.9146\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.2120 - acc: 0.9717 - val_loss: 0.4302 - val_acc: 0.9146\n",
            "Learning rate:  1e-05\n",
            "Epoch 122/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9739Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4826 - acc: 0.9178\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.2057 - acc: 0.9739 - val_loss: 0.4194 - val_acc: 0.9178\n",
            "Learning rate:  1e-05\n",
            "Epoch 123/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9765Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4807 - acc: 0.9174\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1995 - acc: 0.9765 - val_loss: 0.4179 - val_acc: 0.9174\n",
            "Learning rate:  1e-05\n",
            "Epoch 124/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9761Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 207us/sample - loss: 0.4784 - acc: 0.9168\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1992 - acc: 0.9760 - val_loss: 0.4180 - val_acc: 0.9168\n",
            "Learning rate:  1e-05\n",
            "Epoch 125/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9764Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 207us/sample - loss: 0.4869 - acc: 0.9169\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1993 - acc: 0.9764 - val_loss: 0.4167 - val_acc: 0.9169\n",
            "Learning rate:  1e-05\n",
            "Epoch 126/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9776Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4766 - acc: 0.9177\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1951 - acc: 0.9776 - val_loss: 0.4147 - val_acc: 0.9177\n",
            "Learning rate:  1e-05\n",
            "Epoch 127/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9789Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 207us/sample - loss: 0.4718 - acc: 0.9177\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1928 - acc: 0.9789 - val_loss: 0.4141 - val_acc: 0.9177\n",
            "Learning rate:  1e-05\n",
            "Epoch 128/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9778Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4680 - acc: 0.9174\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1930 - acc: 0.9778 - val_loss: 0.4161 - val_acc: 0.9174\n",
            "Learning rate:  1e-05\n",
            "Epoch 129/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9782Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 194us/sample - loss: 0.4458 - acc: 0.9167\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1940 - acc: 0.9782 - val_loss: 0.4169 - val_acc: 0.9167\n",
            "Learning rate:  1e-05\n",
            "Epoch 130/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9780Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.4510 - acc: 0.9171\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1945 - acc: 0.9780 - val_loss: 0.4161 - val_acc: 0.9171\n",
            "Learning rate:  1e-05\n",
            "Epoch 131/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9774Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.4604 - acc: 0.9169\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1944 - acc: 0.9774 - val_loss: 0.4167 - val_acc: 0.9169\n",
            "Learning rate:  1e-05\n",
            "Epoch 132/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1919 - acc: 0.9775Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4528 - acc: 0.9175\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1920 - acc: 0.9775 - val_loss: 0.4167 - val_acc: 0.9175\n",
            "Learning rate:  1e-05\n",
            "Epoch 133/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9786Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4650 - acc: 0.9170\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1932 - acc: 0.9786 - val_loss: 0.4172 - val_acc: 0.9170\n",
            "Learning rate:  1e-05\n",
            "Epoch 134/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9805Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 191us/sample - loss: 0.4554 - acc: 0.9170\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1876 - acc: 0.9805 - val_loss: 0.4178 - val_acc: 0.9170\n",
            "Learning rate:  1e-05\n",
            "Epoch 135/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9791Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.4425 - acc: 0.9179\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1904 - acc: 0.9792 - val_loss: 0.4172 - val_acc: 0.9179\n",
            "Learning rate:  1e-05\n",
            "Epoch 136/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9782Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 194us/sample - loss: 0.4597 - acc: 0.9169\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1913 - acc: 0.9782 - val_loss: 0.4181 - val_acc: 0.9169\n",
            "Learning rate:  1e-05\n",
            "Epoch 137/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9790Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4562 - acc: 0.9166\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1907 - acc: 0.9790 - val_loss: 0.4185 - val_acc: 0.9166\n",
            "Learning rate:  1e-05\n",
            "Epoch 138/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9798Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4585 - acc: 0.9176\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1888 - acc: 0.9798 - val_loss: 0.4195 - val_acc: 0.9176\n",
            "Learning rate:  1e-05\n",
            "Epoch 139/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9789Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.4695 - acc: 0.9172\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1883 - acc: 0.9789 - val_loss: 0.4165 - val_acc: 0.9172\n",
            "Learning rate:  1e-05\n",
            "Epoch 140/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9796Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 194us/sample - loss: 0.4634 - acc: 0.9160\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1875 - acc: 0.9796 - val_loss: 0.4210 - val_acc: 0.9160\n",
            "Learning rate:  1e-05\n",
            "Epoch 141/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9793Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4659 - acc: 0.9179\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1875 - acc: 0.9793 - val_loss: 0.4175 - val_acc: 0.9179\n",
            "Learning rate:  1e-05\n",
            "Epoch 142/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9800Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4633 - acc: 0.9180\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1861 - acc: 0.9800 - val_loss: 0.4199 - val_acc: 0.9180\n",
            "Learning rate:  1e-05\n",
            "Epoch 143/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9806Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4537 - acc: 0.9182\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1850 - acc: 0.9806 - val_loss: 0.4181 - val_acc: 0.9182\n",
            "Learning rate:  1e-05\n",
            "Epoch 144/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9795Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 194us/sample - loss: 0.4628 - acc: 0.9175\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1870 - acc: 0.9795 - val_loss: 0.4211 - val_acc: 0.9175\n",
            "Learning rate:  1e-05\n",
            "Epoch 145/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9801Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.4803 - acc: 0.9167\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1848 - acc: 0.9801 - val_loss: 0.4194 - val_acc: 0.9167\n",
            "Learning rate:  1e-05\n",
            "Epoch 146/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9802Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 194us/sample - loss: 0.4817 - acc: 0.9174\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1867 - acc: 0.9801 - val_loss: 0.4184 - val_acc: 0.9174\n",
            "Learning rate:  1e-05\n",
            "Epoch 147/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9801Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4718 - acc: 0.9167\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1849 - acc: 0.9801 - val_loss: 0.4196 - val_acc: 0.9167\n",
            "Learning rate:  1e-05\n",
            "Epoch 148/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9800Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4573 - acc: 0.9171\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1864 - acc: 0.9800 - val_loss: 0.4201 - val_acc: 0.9171\n",
            "Learning rate:  1e-05\n",
            "Epoch 149/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9802Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 205us/sample - loss: 0.4776 - acc: 0.9174\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1832 - acc: 0.9803 - val_loss: 0.4197 - val_acc: 0.9174\n",
            "Learning rate:  1e-05\n",
            "Epoch 150/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9799Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4705 - acc: 0.9169\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1852 - acc: 0.9799 - val_loss: 0.4205 - val_acc: 0.9169\n",
            "Learning rate:  1e-05\n",
            "Epoch 151/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9795Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.4687 - acc: 0.9167\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1846 - acc: 0.9795 - val_loss: 0.4217 - val_acc: 0.9167\n",
            "Learning rate:  1e-05\n",
            "Epoch 152/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9796Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 206us/sample - loss: 0.4764 - acc: 0.9171\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1843 - acc: 0.9796 - val_loss: 0.4186 - val_acc: 0.9171\n",
            "Learning rate:  1e-05\n",
            "Epoch 153/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9802Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.4839 - acc: 0.9158\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1844 - acc: 0.9803 - val_loss: 0.4217 - val_acc: 0.9158\n",
            "Learning rate:  1e-05\n",
            "Epoch 154/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9800Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 192us/sample - loss: 0.4652 - acc: 0.9168\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1839 - acc: 0.9800 - val_loss: 0.4209 - val_acc: 0.9168\n",
            "Learning rate:  1e-05\n",
            "Epoch 155/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9800Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4662 - acc: 0.9172\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1826 - acc: 0.9800 - val_loss: 0.4193 - val_acc: 0.9172\n",
            "Learning rate:  1e-05\n",
            "Epoch 156/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9801Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.4613 - acc: 0.9165\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1828 - acc: 0.9801 - val_loss: 0.4223 - val_acc: 0.9165\n",
            "Learning rate:  1e-05\n",
            "Epoch 157/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9809Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.4801 - acc: 0.9168\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1819 - acc: 0.9809 - val_loss: 0.4218 - val_acc: 0.9168\n",
            "Learning rate:  1e-05\n",
            "Epoch 158/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9803Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4733 - acc: 0.9168\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1830 - acc: 0.9803 - val_loss: 0.4223 - val_acc: 0.9168\n",
            "Learning rate:  1e-05\n",
            "Epoch 159/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9797Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 192us/sample - loss: 0.4719 - acc: 0.9164\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1832 - acc: 0.9797 - val_loss: 0.4226 - val_acc: 0.9164\n",
            "Learning rate:  1e-05\n",
            "Epoch 160/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9809Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4584 - acc: 0.9162\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1824 - acc: 0.9809 - val_loss: 0.4224 - val_acc: 0.9162\n",
            "Learning rate:  1e-05\n",
            "Epoch 161/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9804Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4637 - acc: 0.9146\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1821 - acc: 0.9805 - val_loss: 0.4230 - val_acc: 0.9146\n",
            "Learning rate:  1e-06\n",
            "Epoch 162/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9811Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4658 - acc: 0.9155\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1815 - acc: 0.9811 - val_loss: 0.4228 - val_acc: 0.9155\n",
            "Learning rate:  1e-06\n",
            "Epoch 163/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9817Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.4672 - acc: 0.9158\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1786 - acc: 0.9816 - val_loss: 0.4198 - val_acc: 0.9158\n",
            "Learning rate:  1e-06\n",
            "Epoch 164/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9817Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4713 - acc: 0.9151\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1789 - acc: 0.9817 - val_loss: 0.4211 - val_acc: 0.9151\n",
            "Learning rate:  1e-06\n",
            "Epoch 165/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9816Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 208us/sample - loss: 0.4655 - acc: 0.9162\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1795 - acc: 0.9817 - val_loss: 0.4209 - val_acc: 0.9162\n",
            "Learning rate:  1e-06\n",
            "Epoch 166/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9813Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4650 - acc: 0.9166\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1807 - acc: 0.9813 - val_loss: 0.4211 - val_acc: 0.9166\n",
            "Learning rate:  1e-06\n",
            "Epoch 167/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9804Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4687 - acc: 0.9159\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1805 - acc: 0.9804 - val_loss: 0.4206 - val_acc: 0.9159\n",
            "Learning rate:  1e-06\n",
            "Epoch 168/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9814Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4752 - acc: 0.9159\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1779 - acc: 0.9813 - val_loss: 0.4229 - val_acc: 0.9159\n",
            "Learning rate:  1e-06\n",
            "Epoch 169/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9823Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.4658 - acc: 0.9163\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1774 - acc: 0.9823 - val_loss: 0.4210 - val_acc: 0.9163\n",
            "Learning rate:  1e-06\n",
            "Epoch 170/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9811Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 193us/sample - loss: 0.4692 - acc: 0.9159\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1796 - acc: 0.9811 - val_loss: 0.4212 - val_acc: 0.9159\n",
            "Learning rate:  1e-06\n",
            "Epoch 171/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9820Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 193us/sample - loss: 0.4672 - acc: 0.9169\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1782 - acc: 0.9820 - val_loss: 0.4196 - val_acc: 0.9169\n",
            "Learning rate:  1e-06\n",
            "Epoch 172/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9805Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 189us/sample - loss: 0.4716 - acc: 0.9169\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1810 - acc: 0.9805 - val_loss: 0.4199 - val_acc: 0.9169\n",
            "Learning rate:  1e-06\n",
            "Epoch 173/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9816Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 188us/sample - loss: 0.4678 - acc: 0.9157\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1789 - acc: 0.9815 - val_loss: 0.4215 - val_acc: 0.9157\n",
            "Learning rate:  1e-06\n",
            "Epoch 174/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9821Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 197us/sample - loss: 0.4699 - acc: 0.9154\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1788 - acc: 0.9821 - val_loss: 0.4223 - val_acc: 0.9154\n",
            "Learning rate:  1e-06\n",
            "Epoch 175/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9811Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.4675 - acc: 0.9159\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1804 - acc: 0.9811 - val_loss: 0.4221 - val_acc: 0.9159\n",
            "Learning rate:  1e-06\n",
            "Epoch 176/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9812Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4666 - acc: 0.9155\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1787 - acc: 0.9812 - val_loss: 0.4215 - val_acc: 0.9155\n",
            "Learning rate:  1e-06\n",
            "Epoch 177/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9820Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 208us/sample - loss: 0.4670 - acc: 0.9154\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1778 - acc: 0.9820 - val_loss: 0.4214 - val_acc: 0.9154\n",
            "Learning rate:  1e-06\n",
            "Epoch 178/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9811Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 0.4651 - acc: 0.9156\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1797 - acc: 0.9811 - val_loss: 0.4206 - val_acc: 0.9156\n",
            "Learning rate:  1e-06\n",
            "Epoch 179/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9813Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4705 - acc: 0.9154\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1798 - acc: 0.9813 - val_loss: 0.4225 - val_acc: 0.9154\n",
            "Learning rate:  1e-06\n",
            "Epoch 180/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9810Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 206us/sample - loss: 0.4701 - acc: 0.9162\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1797 - acc: 0.9810 - val_loss: 0.4206 - val_acc: 0.9162\n",
            "Learning rate:  1e-06\n",
            "Epoch 181/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9822Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.4663 - acc: 0.9157\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1765 - acc: 0.9822 - val_loss: 0.4207 - val_acc: 0.9157\n",
            "Learning rate:  5e-07\n",
            "Epoch 182/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9816Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 188us/sample - loss: 0.4639 - acc: 0.9168\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1792 - acc: 0.9817 - val_loss: 0.4200 - val_acc: 0.9168\n",
            "Learning rate:  5e-07\n",
            "Epoch 183/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9820Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4676 - acc: 0.9155\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1788 - acc: 0.9819 - val_loss: 0.4218 - val_acc: 0.9155\n",
            "Learning rate:  5e-07\n",
            "Epoch 184/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9809Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 189us/sample - loss: 0.4636 - acc: 0.9163\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1806 - acc: 0.9809 - val_loss: 0.4208 - val_acc: 0.9163\n",
            "Learning rate:  5e-07\n",
            "Epoch 185/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9820Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4635 - acc: 0.9157\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1764 - acc: 0.9820 - val_loss: 0.4217 - val_acc: 0.9157\n",
            "Learning rate:  5e-07\n",
            "Epoch 186/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9809Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4630 - acc: 0.9160\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1790 - acc: 0.9809 - val_loss: 0.4194 - val_acc: 0.9160\n",
            "Learning rate:  5e-07\n",
            "Epoch 187/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9812Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4654 - acc: 0.9153\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1782 - acc: 0.9813 - val_loss: 0.4224 - val_acc: 0.9153\n",
            "Learning rate:  5e-07\n",
            "Epoch 188/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9804Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4685 - acc: 0.9158\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1796 - acc: 0.9805 - val_loss: 0.4207 - val_acc: 0.9158\n",
            "Learning rate:  5e-07\n",
            "Epoch 189/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9813Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.4702 - acc: 0.9162\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1803 - acc: 0.9812 - val_loss: 0.4223 - val_acc: 0.9162\n",
            "Learning rate:  5e-07\n",
            "Epoch 190/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9813Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 0.4670 - acc: 0.9164\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1789 - acc: 0.9813 - val_loss: 0.4216 - val_acc: 0.9164\n",
            "Learning rate:  5e-07\n",
            "Epoch 191/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9811Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 225us/sample - loss: 0.4684 - acc: 0.9164\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 0.1792 - acc: 0.9811 - val_loss: 0.4200 - val_acc: 0.9164\n",
            "Learning rate:  5e-07\n",
            "Epoch 192/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9818Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4659 - acc: 0.9159\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1772 - acc: 0.9818 - val_loss: 0.4209 - val_acc: 0.9159\n",
            "Learning rate:  5e-07\n",
            "Epoch 193/200\n",
            "1561/1562 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9817Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 193us/sample - loss: 0.4644 - acc: 0.9161\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 25ms/step - loss: 0.1784 - acc: 0.9817 - val_loss: 0.4205 - val_acc: 0.9161\n",
            "Learning rate:  5e-07\n",
            "Epoch 194/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9812Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4657 - acc: 0.9163\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1796 - acc: 0.9812 - val_loss: 0.4198 - val_acc: 0.9163\n",
            "Learning rate:  5e-07\n",
            "Epoch 195/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9812Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4646 - acc: 0.9151\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1798 - acc: 0.9812 - val_loss: 0.4221 - val_acc: 0.9151\n",
            "Learning rate:  5e-07\n",
            "Epoch 196/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9811Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 201us/sample - loss: 0.4645 - acc: 0.9163\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1780 - acc: 0.9812 - val_loss: 0.4196 - val_acc: 0.9163\n",
            "Learning rate:  5e-07\n",
            "Epoch 197/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9813Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 199us/sample - loss: 0.4663 - acc: 0.9161\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1785 - acc: 0.9813 - val_loss: 0.4204 - val_acc: 0.9161\n",
            "Learning rate:  5e-07\n",
            "Epoch 198/200\n",
            "1559/1562 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9822Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 0.4647 - acc: 0.9155\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1776 - acc: 0.9822 - val_loss: 0.4207 - val_acc: 0.9155\n",
            "Learning rate:  5e-07\n",
            "Epoch 199/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9811Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 202us/sample - loss: 0.4680 - acc: 0.9155\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1802 - acc: 0.9811 - val_loss: 0.4210 - val_acc: 0.9155\n",
            "Learning rate:  5e-07\n",
            "Epoch 200/200\n",
            "1560/1562 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9821Epoch 1/200\n",
            "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 0.4640 - acc: 0.9157\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "1562/1562 [==============================] - 38s 24ms/step - loss: 0.1777 - acc: 0.9821 - val_loss: 0.4204 - val_acc: 0.9157\n",
            "Test loss: 0.42018798842430116\n",
            "Test accuracy: 0.9157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlYWCdJTpRz6",
        "colab_type": "text"
      },
      "source": [
        "## ResNet v2 - 개선된 ResNet 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZkQaT7CpR3D",
        "colab_type": "text"
      },
      "source": [
        "### ResNet v1 과 ResNet v2 비교"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tatb2wSc4ii9",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605043-8f72ab80-45dc-11ea-8a66-530581ea7d76.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6nQYlZl5DYs",
        "colab_type": "text"
      },
      "source": [
        "* 연산량을 줄이기 위해 1X1-3X3-1X1 BN=ReLU-Conv2D 스택을 사용 - 병목 레이어 (bottleneck layer)\n",
        "* 배치 정규화와 ReLU 활성화 함수를 2D 합성곱 전에 위치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBIWENKX5Ddg",
        "colab_type": "text"
      },
      "source": [
        "## 밀집 연결 합성곱 네트워크(DenseNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up1GS3AZ651F",
        "colab_type": "text"
      },
      "source": [
        "Dense는 ResNet과 달리 숏컷 연결이 아닌 이전 특징 맵 전체가 다음 계층의 입력이 되는 방식으로 그레디언트 소실문제를 해결한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPg9cni68vfn",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605264-833c1d80-45df-11ea-976d-d6d88cb731fd.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWLwBIJe8vnM",
        "colab_type": "text"
      },
      "source": [
        "Conv2D 역시 크기가 3인 커널을 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HlCadKg-2R9",
        "colab_type": "text"
      },
      "source": [
        "계층마다 생성되는 특징 맵의 개수를 성장률$k$라고 한다.\n",
        "<br>일반적으로 $k$=12를 사용한다. 특정 맵 $x_0$ 개수가 $k_0$일 때 위 그림처럼 4개의 계층으로 구성된 밀집 계층의 끝에서 특징 맵의 전체 개수는 $4 \\times k +k_0 $이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOMrUSLFAFBE",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605405-2b9eb180-45e1-11ea-9d25-a247de91a561.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeHCfh0VAmfo",
        "colab_type": "text"
      },
      "source": [
        "병합 연산에서 연결(Concat)을 사용하기 위해 크기의 차이를 일치하여야 한다. 특징 맵 개수가 효율적인 개수까지 증가하도록 하기위해 우측 그림처럼 병목 계층을 도입했다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8GV4qVqBXjM",
        "colab_type": "text"
      },
      "source": [
        "연결 후에는 필터 크기가 $4k$인 $1 \\times 1$ 합성곱이 적용된다.\n",
        "<br> 이 것은 Conv2D(3)에서 처리되는 특징 맵 개수가 빠르게 증가하는 것을 막는다.\n",
        "\n",
        "<br> 병목 도입을 안할 떈 Conv2D(3)에서 받는 입력은 1224개고\n",
        "병목계층을 도입한 후에는 48개의 특징 맵만 입력받는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9wM2aNYBm9C",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605482-33128a80-45e2-11ea-9290-261585335e8e.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCezasybCSRz",
        "colab_type": "text"
      },
      "source": [
        "특징 맵 크기는 서로 다 다르기 때문에 이를 해결하기 위해 DenseNet은 심층 신경망을 여러 개의 밀집 블록으로 나누고 전이 계층을 통해 서로 연결시킨다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m28NMnC1CbLk",
        "colab_type": "text"
      },
      "source": [
        "각 밀집 블록 내에서는 특징 맵 크기(너비, 높이)가 일정하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXmMi62dCgRE",
        "colab_type": "text"
      },
      "source": [
        "전이 계층 : 두 밀집 블록 사이에서 한 특징 맵 크기에서 더 작은 특징 맵 크기로 바꾸는 것 (일반적으론 절반으로 줄인다)\n",
        "\n",
        "<br> 위 기능은 AveragePooling2D 계층이 수행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF0BcDWYC0dB",
        "colab_type": "text"
      },
      "source": [
        "예를 들면, 기본 pool_size=2 를 적용한 AveragePooling2D를 사용하면 맵 크기가 (64,64,256)에서 (32,32,256)으로 줄어든다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHcp_1HnDEek",
        "colab_type": "text"
      },
      "source": [
        "하지만, 특징 맵이 AveragePooling2D 으로 전달되기 전에 Conv2D(1)을 사용해 맵 개수를 특정 압축비 $0<\\theta<1$로 줄인다. 여기선 0.5로 정한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toGhv60mC0gT",
        "colab_type": "text"
      },
      "source": [
        "따라서 (64,64,512)의 출력이 Conv2D에서 (64,64,256)으로 줄어들고, AveragePooling2D를 통해 (32,32,256)으로 한번더 줄어드는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHR2_0_eDtnk",
        "colab_type": "text"
      },
      "source": [
        "### CIFAR10을 위한 100계층 DenseNet-BC 구성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWcfeFBkD0lc",
        "colab_type": "text"
      },
      "source": [
        "이제 CIFAR10 데이터 세트에 100개의 계층으로 구성된 DenseNet-BC(병목 압축)을 구성한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4HriTX3D_P6",
        "colab_type": "text"
      },
      "source": [
        "아래 표는 100개의 계층으로 이루어진 DenseNet-BC이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFfgeKJpEJeE",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605581-6275c700-45e3-11ea-8ba9-256081c85a5c.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16xwX57-EJiR",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/73605596-93ee9280-45e3-11ea-8c40-9d071d9cf6fb.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gm_f-SLEKFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Input, Flatten, Dropout\n",
        "from tensorflow.keras.layers import concatenate, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# 훈련 매개변수\n",
        "batch_size = 32\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "\n",
        "# 네트워크 매개변수\n",
        "num_classes = 10\n",
        "num_dense_blocks = 3\n",
        "use_max_pool = False\n",
        "\n",
        "# DenseNet-BC 데이터 셋\n",
        "# Growth rate   | Depth |  Accuracy (paper)| Accuracy (this)      |\n",
        "# 12            | 100   |  95.49%          | 93.74%               |\n",
        "# 24            | 250   |  96.38%          | requires big mem GPU |\n",
        "# 40            | 190   |  96.54%          | requires big mem GPU |\n",
        "growth_rate = 12\n",
        "depth = 100\n",
        "num_bottleneck_layers = (depth - 4) // (2 * num_dense_blocks)\n",
        "\n",
        "num_filters_bef_dense_block = 2 * growth_rate\n",
        "compression_factor = 0.5\n",
        "\n",
        "# CIFAR10 데이터 불러오기\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# 데이터 정규화\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "# 모델 정의\n",
        "# densenet CNNs (합성함수)는 BN-ReLU-Conv2D로 구성됨\n",
        "inputs = Input(shape=input_shape)\n",
        "x = BatchNormalization()(inputs)\n",
        "x = Activation('relu')(x)\n",
        "x = Conv2D(num_filters_bef_dense_block,\n",
        "           kernel_size=3,\n",
        "           padding='same',\n",
        "           kernel_initializer='he_normal')(x)\n",
        "x = concatenate([inputs, x])\n",
        "\n",
        "# 전이층으로 연결된 밀집 블록의 스택\n",
        "for i in range(num_dense_blocks):\n",
        "    # 밀집 블록은 병목 계층의 스택이다\n",
        "    for j in range(num_bottleneck_layers):\n",
        "        y = BatchNormalization()(x)\n",
        "        y = Activation('relu')(y)\n",
        "        y = Conv2D(4 * growth_rate,\n",
        "                   kernel_size=1,\n",
        "                   padding='same',\n",
        "                   kernel_initializer='he_normal')(y)\n",
        "        if not data_augmentation:\n",
        "            y = Dropout(0.2)(y)\n",
        "        y = BatchNormalization()(y)\n",
        "        y = Activation('relu')(y)\n",
        "        y = Conv2D(growth_rate,\n",
        "                   kernel_size=3,\n",
        "                   padding='same',\n",
        "                   kernel_initializer='he_normal')(y)\n",
        "        if not data_augmentation:\n",
        "            y = Dropout(0.2)(y)\n",
        "        x = concatenate([x, y])\n",
        "\n",
        "    # 마미작 밀집 블록 다음에는 전이 계층이 없다\n",
        "    if i == num_dense_blocks - 1:\n",
        "        continue\n",
        "\n",
        "    # 전이 계층에서 특징 맵 개수를 압축하고 크기를 1/2로 줄인다.\n",
        "    num_filters_bef_dense_block += num_bottleneck_layers * growth_rate\n",
        "    num_filters_bef_dense_block = int(num_filters_bef_dense_block * compression_factor)\n",
        "    y = BatchNormalization()(x)\n",
        "    y = Conv2D(num_filters_bef_dense_block,\n",
        "               kernel_size=1,\n",
        "               padding='same',\n",
        "               kernel_initializer='he_normal')(y)\n",
        "    if not data_augmentation:\n",
        "        y = Dropout(0.2)(y)\n",
        "    x = AveragePooling2D()(y)\n",
        "\n",
        "\n",
        "# 애버리지 풀링 다음으로 상단에 분류 모델 추가\n",
        "# 특징 맵 크기는1 x 1\n",
        "x = AveragePooling2D(pool_size=8)(x)\n",
        "y = Flatten()(x)\n",
        "outputs = Dense(num_classes,\n",
        "                kernel_initializer='he_normal',\n",
        "                activation='softmax')(y)\n",
        "\n",
        "# 최초 논문에서는 SGD를 사용했지만 DenseNet에서는 RMSprop 가 성능이 좋다.\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(1e-3),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "plot_model(model, to_file=\"cifar10-densenet.png\", show_shapes=True)\n",
        "\n",
        "# prepare model model saving directory\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_densenet_model.{epoch:02d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# prepare callbacks for model saving and for learning rate reducer\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# run training, with or without data augmentation\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # preprocessing  and realtime data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (deg 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally\n",
        "        height_shift_range=0.1,  # randomly shift images vertically\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "    # compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied)\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # fit the model on the batches generated by datagen.flow()\n",
        "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# score trained model\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}